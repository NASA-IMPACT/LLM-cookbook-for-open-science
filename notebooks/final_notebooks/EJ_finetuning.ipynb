{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The aim of this notebook is to set up a tutorial for finetuning metadata extraction process for Environmental Justice Datasets\n",
    "## installing the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai==1.12.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.12.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openai==1.12.0) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openai==1.12.0) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openai==1.12.0) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openai==1.12.0) (2.6.4)\n",
      "Requirement already satisfied: sniffio in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openai==1.12.0) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openai==1.12.0) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openai==1.12.0) (4.9.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai==1.12.0) (3.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/rajashreedahal/Library/Python/3.10/lib/python/site-packages (from anyio<5,>=3.5.0->openai==1.12.0) (1.2.0)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai==1.12.0) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai==1.12.0) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.12.0) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai==1.12.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai==1.12.0) (2.16.3)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.2.1)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/rajashreedahal/Library/Python/3.10/lib/python/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: python-dotenv in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.0.1)\n",
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nltk) (4.66.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai==1.12.0\n",
    "!pip install pandas\n",
    "!pip install beautifulsoup4\n",
    "!pip install python-dotenv\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/rajashreedahal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rajashreedahal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ssl\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Download NLTK resources\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading the Environmental Justice Dataset from S3 bucket and looking at the content in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://llmworkshop/EJ_datasets.csv to ./EJ_datasets.csv     \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Indicators</th>\n",
       "      <th>Description</th>\n",
       "      <th>Description Simplified</th>\n",
       "      <th>Geographic Coverage</th>\n",
       "      <th>Format</th>\n",
       "      <th>Spatial Resolution</th>\n",
       "      <th>Spatial Resolution (Standard)</th>\n",
       "      <th>Temporal Resolution</th>\n",
       "      <th>Temporal Extent</th>\n",
       "      <th>Latency</th>\n",
       "      <th>Source/Link</th>\n",
       "      <th>Project</th>\n",
       "      <th>Strengths</th>\n",
       "      <th>Limitations</th>\n",
       "      <th>Data Visualization</th>\n",
       "      <th>Intended Use</th>\n",
       "      <th>Tab Name Indicator</th>\n",
       "      <th>Indicators                     (Select from drop-down list)</th>\n",
       "      <th>sde_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABoVE: Landsat-derived Burn Scar dNBR across A...</td>\n",
       "      <td>Disasters</td>\n",
       "      <td>This dataset contains differenced Normalized B...</td>\n",
       "      <td>The ABoVE: Landsat-derived Burn Scar dNBR acro...</td>\n",
       "      <td>Alaska, Canada</td>\n",
       "      <td>GeoTIFF</td>\n",
       "      <td>30 meters</td>\n",
       "      <td>30 meters</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1985-01-01 to 2015-12-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://dx.doi.org/10.3334/ORNLDAAC/1564</td>\n",
       "      <td>ABoVE - Arctic-Boreal Vulnerability Experiment</td>\n",
       "      <td>30-year temporal extent</td>\n",
       "      <td>Lacks recent data</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Path C</td>\n",
       "      <td>Disasters</td>\n",
       "      <td>Disasters</td>\n",
       "      <td>https://sciencediscoveryengine.nasa.gov/app/na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AERONET Ground-based AOD Measurements</td>\n",
       "      <td>Health &amp; Air Quality</td>\n",
       "      <td>The AERONET Data Display Interface allows user...</td>\n",
       "      <td>The AERONET Data Display Interface allows user...</td>\n",
       "      <td>Global</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1993-01-01 to Present</td>\n",
       "      <td>NRT</td>\n",
       "      <td>https://aeronet.gsfc.nasa.gov/cgi-bin/draw_map...</td>\n",
       "      <td>AERONET - Aerosol Robotic Network</td>\n",
       "      <td>NRT, 19-year temporal extent</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Map Viewer</td>\n",
       "      <td>Path C</td>\n",
       "      <td>Health &amp; Air Quality</td>\n",
       "      <td>Health &amp; Air Quality</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ARIA (Advanced Rapid Imaging and Analysis) DPM...</td>\n",
       "      <td>Disasters,Urban Flooding</td>\n",
       "      <td>The Advanced Rapid Imaging and Analysis (ARIA)...</td>\n",
       "      <td>The ARIA (Advanced Rapid Imaging and Analysis)...</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>KML</td>\n",
       "      <td>30 meters</td>\n",
       "      <td>30 meters</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-03-25, 2017-09-21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://appliedsciences.nasa.gov/our-impact/ne...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Visualization available through ArcGIS viewer</td>\n",
       "      <td>Data only available for case study of Puerto R...</td>\n",
       "      <td>ArcGIS viewer</td>\n",
       "      <td>Path B</td>\n",
       "      <td>Urban Flooding</td>\n",
       "      <td>Disasters</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ARIA (Advanced Rapid Imaging and Analysis) Dat...</td>\n",
       "      <td>Disasters</td>\n",
       "      <td>The ARIA Project, a joint effort of the Califo...</td>\n",
       "      <td>The ARIA Project, a joint effort of the Califo...</td>\n",
       "      <td>Global</td>\n",
       "      <td>GeoTIFF, KMZ</td>\n",
       "      <td>Varies</td>\n",
       "      <td>Varies</td>\n",
       "      <td>Varies</td>\n",
       "      <td>Varies</td>\n",
       "      <td>NRT</td>\n",
       "      <td>https://aria-share.jpl.nasa.gov/</td>\n",
       "      <td>ARIA - Advanced Rapid Imaging and Analysis</td>\n",
       "      <td>NRT</td>\n",
       "      <td>No visualization available</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Path C</td>\n",
       "      <td>Disasters</td>\n",
       "      <td>Disasters</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ASTER L2 Surface Temperature V003</td>\n",
       "      <td>Climate Change,Extreme Heat</td>\n",
       "      <td>The ASTER Surface Kinetic Temperature (AST_08)...</td>\n",
       "      <td>The ASTER Surface Kinetic Temperature (AST_08)...</td>\n",
       "      <td>Global</td>\n",
       "      <td>HDF-EOS2</td>\n",
       "      <td>90 m</td>\n",
       "      <td>90 meters</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000-03-04 to Present</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://dx.doi.org/10.5067/ASTER/AST_08.003</td>\n",
       "      <td>Terra - Earth Observing System (EOS), Terra</td>\n",
       "      <td>22-year temporal extent</td>\n",
       "      <td>Only HDF-EOS2 format available</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Path C</td>\n",
       "      <td>Climate Change</td>\n",
       "      <td>Extreme Heat</td>\n",
       "      <td>https://sciencediscoveryengine.nasa.gov/app/na...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Dataset  \\\n",
       "0  ABoVE: Landsat-derived Burn Scar dNBR across A...   \n",
       "1              AERONET Ground-based AOD Measurements   \n",
       "2  ARIA (Advanced Rapid Imaging and Analysis) DPM...   \n",
       "3  ARIA (Advanced Rapid Imaging and Analysis) Dat...   \n",
       "4                  ASTER L2 Surface Temperature V003   \n",
       "\n",
       "                    Indicators  \\\n",
       "0                    Disasters   \n",
       "1         Health & Air Quality   \n",
       "2     Disasters,Urban Flooding   \n",
       "3                    Disasters   \n",
       "4  Climate Change,Extreme Heat   \n",
       "\n",
       "                                         Description  \\\n",
       "0  This dataset contains differenced Normalized B...   \n",
       "1  The AERONET Data Display Interface allows user...   \n",
       "2  The Advanced Rapid Imaging and Analysis (ARIA)...   \n",
       "3  The ARIA Project, a joint effort of the Califo...   \n",
       "4  The ASTER Surface Kinetic Temperature (AST_08)...   \n",
       "\n",
       "                              Description Simplified Geographic Coverage  \\\n",
       "0  The ABoVE: Landsat-derived Burn Scar dNBR acro...      Alaska, Canada   \n",
       "1  The AERONET Data Display Interface allows user...              Global   \n",
       "2  The ARIA (Advanced Rapid Imaging and Analysis)...         Puerto Rico   \n",
       "3  The ARIA Project, a joint effort of the Califo...              Global   \n",
       "4  The ASTER Surface Kinetic Temperature (AST_08)...              Global   \n",
       "\n",
       "         Format Spatial Resolution Spatial Resolution (Standard)  \\\n",
       "0       GeoTIFF          30 meters                     30 meters   \n",
       "1           NaN                NaN                           NaN   \n",
       "2           KML          30 meters                     30 meters   \n",
       "3  GeoTIFF, KMZ             Varies                        Varies   \n",
       "4      HDF-EOS2               90 m                     90 meters   \n",
       "\n",
       "  Temporal Resolution           Temporal Extent Latency  \\\n",
       "0                 NaN  1985-01-01 to 2015-12-31     NaN   \n",
       "1                 NaN     1993-01-01 to Present     NRT   \n",
       "2                 NaN    2017-03-25, 2017-09-21     NaN   \n",
       "3              Varies                    Varies     NRT   \n",
       "4                 NaN     2000-03-04 to Present     NaN   \n",
       "\n",
       "                                         Source/Link  \\\n",
       "0           https://dx.doi.org/10.3334/ORNLDAAC/1564   \n",
       "1  https://aeronet.gsfc.nasa.gov/cgi-bin/draw_map...   \n",
       "2  https://appliedsciences.nasa.gov/our-impact/ne...   \n",
       "3                   https://aria-share.jpl.nasa.gov/   \n",
       "4        https://dx.doi.org/10.5067/ASTER/AST_08.003   \n",
       "\n",
       "                                          Project  \\\n",
       "0  ABoVE - Arctic-Boreal Vulnerability Experiment   \n",
       "1               AERONET - Aerosol Robotic Network   \n",
       "2                                             NaN   \n",
       "3      ARIA - Advanced Rapid Imaging and Analysis   \n",
       "4     Terra - Earth Observing System (EOS), Terra   \n",
       "\n",
       "                                       Strengths  \\\n",
       "0                        30-year temporal extent   \n",
       "1                   NRT, 19-year temporal extent   \n",
       "2  Visualization available through ArcGIS viewer   \n",
       "3                                            NRT   \n",
       "4                        22-year temporal extent   \n",
       "\n",
       "                                         Limitations Data Visualization  \\\n",
       "0                                  Lacks recent data                NaN   \n",
       "1                                                NaN         Map Viewer   \n",
       "2  Data only available for case study of Puerto R...      ArcGIS viewer   \n",
       "3                         No visualization available                NaN   \n",
       "4                     Only HDF-EOS2 format available                NaN   \n",
       "\n",
       "  Intended Use    Tab Name Indicator  \\\n",
       "0       Path C             Disasters   \n",
       "1       Path C  Health & Air Quality   \n",
       "2       Path B        Urban Flooding   \n",
       "3       Path C             Disasters   \n",
       "4       Path C        Climate Change   \n",
       "\n",
       "  Indicators                     (Select from drop-down list)  \\\n",
       "0                                          Disasters            \n",
       "1                               Health & Air Quality            \n",
       "2                                          Disasters            \n",
       "3                                          Disasters            \n",
       "4                                       Extreme Heat            \n",
       "\n",
       "                                            sde_link  \n",
       "0  https://sciencediscoveryengine.nasa.gov/app/na...  \n",
       "1                                                NaN  \n",
       "2                                                NaN  \n",
       "3                                                NaN  \n",
       "4  https://sciencediscoveryengine.nasa.gov/app/na...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!aws s3 cp s3://llmworkshop/EJ_datasets.csv .\n",
    "train_data=pd.read_csv(\"EJ_datasets.csv\",encoding='latin1')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We will carry out the following preprocessing steps:\n",
    "1. Only consider those source links which have 'doi' in it\n",
    "2. We extract the features \"Indicators\",\"Geographic Coverage\",\"Spatial Resolution\", \"Temporal Resolution\", and \"Temporal Extent\" for our metadata extraction process\n",
    "3. We extract text based response from request for url and preprocess it before feeding it to the model for finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Access the API key from the environment\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "final_data={}\n",
    "for each_data in train_data.iterrows():\n",
    "    if \"doi\" in each_data[1].iloc[11]:\n",
    "        final_data[each_data[1].iloc[11]]={\"Indicators\":each_data[1].iloc[1],\n",
    "                  \"Geographic_Coverage\":each_data[1].iloc[4],\n",
    "                  \"Format\":each_data[1].iloc[5],\n",
    "                  \"Spatial_Resolution\":each_data[1].iloc[7],\n",
    "                  \"Temporal_Resolution\":each_data[1].iloc[8],\n",
    "                  \"Temporal_Extent\":each_data[1].iloc[9]}\n",
    "        \n",
    "for url,value in final_data.items():\n",
    "        response=requests.get(url)\n",
    "        html_page = response.text\n",
    "        soup = BeautifulSoup(html_page, \"html.parser\")\n",
    "        text=soup.get_text()\n",
    "        text=text.lower()\n",
    "        # Tokenization\n",
    "        tokens = word_tokenize(text) \n",
    "        # Remove punctuation and make lowercase\n",
    "        tokens = [word.lower() for word in tokens if word.isalnum()]\n",
    "        # Remove stopwords\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "        text=\" \".join(tokens)\n",
    "        text = re.sub(r'[\\t\\n\\r\\f\\v]+', '', text)\n",
    "        text=re.sub(r'[^\\w\\s]', '', text)\n",
    "        final_data[url][\"text\"]=text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training file preparation steps steps:\n",
    "1. Define a _SYSTEM_PROMPT: This will instruct the gpt model on extraction process. Please go through the details in _SYSTEM_PROMPT to go through the instructions\n",
    "2. data_format: The train data here should be in .jsonl format. Here, each data is in the form :\n",
    "   \n",
    "   {\n",
    "    \"messages\": [\n",
    "   \n",
    "        {\"role\": \"system\", \"content\": \"_SYSTEM_PROMPT\"},\n",
    "   \n",
    "        {\"role\": \"user\", \"content\": preprocessed text from source url},\n",
    "   \n",
    "        {\"role\": \"assistant\", \"content\": \"output_format\"}\n",
    "   \n",
    "    ]\n",
    "}\n",
    "\n",
    "This will instruct the gpt model to be aware about different roles as 'system', 'user', and 'assistant' respectively.\n",
    "\n",
    "3. Create a file at OpenAI for fine-tune purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-DmF8QRHE5l8DLWpZJOp2BIR1', bytes=379274, created_at=1711122028, filename='EJ_datasets_finetuning4.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_SYSTEM_PROMPT =\"Extract metadata and entities details accurately from my requests.\" + \"Metadata extraction for Indicators should clearly be one of the element in the list ['Disasters','Human Dimensions','Food Availability','Health & Air Quality','Water Availability', 'Extreme Heat','Urban Flooding','Climate Change']. Do not create new element for indicators \" + \"Geographic_Coverage: If multiple countries, write global, else write name of location\"+\"Make sure the following fields follow the following regex pattern\"+\"Spatial_Resolution: ^(\\d+(\\.\\d+)? [a-zA-Z]+|varies|N/A)$\"+\"Temporal_Resolution: ^(\\d+(\\.\\d+)? [a-zA-Z]+|N/A|varies|weekly|monthly|daily|yearly|varies-multiple datasets included|Daily < Weekly|Hourly < Daily|Weekly < Monthly|Monthly < Yearly|1 minute)$\"+\"Temporal_Extent: ^(\\d{4}-\\d{2}-\\d{2} to present|present|\\d{4}-\\d{2}-\\d{2} (?:to|until) present|\\d{4}-\\d{2}-\\d{2} to \\d{4}-\\d{2}-\\d{2}|varies(?:- multiple datasets (?:included|available))?)$.\"\n",
    "        \n",
    "\n",
    "train_data=[]\n",
    "for url,value in final_data.items():\n",
    "    output_content={\"Indicators\":value[\"Indicators\"],\"Geographic_Coverage\":value[\"Geographic_Coverage\"], \"Format\":value[\"Format\"],\"Spatial_Resolution\":value[\"Spatial_Resolution\"],\"Temporal_Resolution\":value[\"Temporal_Resolution\"],\"Temporal_Extent\":value[\"Temporal_Extent\"]}\n",
    "    output_content=str(output_content)\n",
    "    data={\"messages\": [{\"role\": \"system\", \"content\":_SYSTEM_PROMPT},{\"role\": \"user\", \"content\":f'{value[\"text\"]}'}, {\"role\": \"assistant\", \"content\": output_content}]}\n",
    "    train_data.append(data)\n",
    "    \n",
    "with open(\"EJ_datasets_finetuning4.jsonl\", 'w') as jsonl_file:\n",
    "    for record in train_data:\n",
    "        jsonl_file.write(json.dumps(record) + '\\n')\n",
    "\n",
    "\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "client.files.create(\n",
    "  file=open(\"EJ_datasets_finetuning4.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, we use the generated fileobject at OpenAI to create a finetuning job using gpt-3.5-turbo-0125 model for 10 epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-4KHnDC5jYAVU38XQfZ9DvDsq', created_at=1711122038, error=Error(code=None, message=None, param=None, error=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs=10, batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-KVpMEM9Q8Xm2pc8YuAMYw8r3', result_files=[], status='validating_files', trained_tokens=None, training_file='file-DmF8QRHE5l8DLWpZJOp2BIR1', validation_file=None, user_provided_suffix=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "client.fine_tuning.jobs.create(\n",
    "  training_file=\"file-DmF8QRHE5l8DLWpZJOp2BIR1\", \n",
    "  model=\"gpt-3.5-turbo-0125\", \n",
    "  hyperparameters={\n",
    "    \"n_epochs\":10\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing finetuning jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SyncCursorPage[FineTuningJob](data=[FineTuningJob(id='ftjob-4KHnDC5jYAVU38XQfZ9DvDsq', created_at=1711122038, error=Error(code=None, message=None, param=None, error=None), fine_tuned_model='ft:gpt-3.5-turbo-0125:nasa::95blPXsF', finished_at=1711123830, hyperparameters=Hyperparameters(n_epochs=10, batch_size=1, learning_rate_multiplier=2), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-KVpMEM9Q8Xm2pc8YuAMYw8r3', result_files=['file-uUDPVa6eHOWoqXJjwDbDzZzN'], status='succeeded', trained_tokens=747980, training_file='file-DmF8QRHE5l8DLWpZJOp2BIR1', validation_file=None, user_provided_suffix=None)], object='list', has_more=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.fine_tuning.jobs.list(limit=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here, the finetuned job id is: ftjob-4KHnDC5jYAVU38XQfZ9DvDsq and finetuned model is: ft:gpt-3.5-turbo-0125:nasa::95blPXsF. We will use this model for metadata extraction process for Environmental Justice datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
