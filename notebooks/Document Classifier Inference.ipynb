{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d0006e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (4.33.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (1.24.2)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.9.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch) (4.8.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.5.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas) (1.24.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers\n",
    "! pip install torch\n",
    "! pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bb50919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import importlib  \n",
    "from torch.utils.data import DataLoader, SequentialSampler, TensorDataset\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Suppress FutureWarning messages\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "config={\n",
    "    \"dataload\": {\"batch_size\": 12},\n",
    "    \"classes\": {\n",
    "        \"Data\": 0,\n",
    "        \"Documentation\": 1,\n",
    "        \"Mission and Instruments\": 2,\n",
    "        \"image\": 3,\n",
    "        \"Software and tools\": 4,\n",
    "        \"Training and Education\": 5\n",
    "    },\n",
    "    \"encoder\": {\n",
    "        \"image_keyword\": [\n",
    "            \"image\",\n",
    "            \"images\",\n",
    "            \"photojournal\",\n",
    "            \"view\",\n",
    "            \"Images\",\n",
    "            \"Related\"\n",
    "        ],\n",
    "        \"software_keyword\": [\n",
    "            \"software\",\n",
    "            \"tools\",\n",
    "            \"SDK\",\n",
    "            \"API\",\n",
    "            \"toolkit\",\n",
    "            \"library\",\n",
    "            \"application\",\n",
    "            \"code\",\n",
    "            \"class\",\n",
    "            \"package\",\n",
    "            \"interface\",\n",
    "            \"annotation\",\n",
    "            \"index\",\n",
    "            \"tree\",\n",
    "            \"constructor\",\n",
    "            \"method\",\n",
    "            \"version\",\n",
    "            \"model\",\n",
    "            \"attributes\",\n",
    "            \"methods\",\n",
    "            \"seed\",\n",
    "            \"version\",\n",
    "            \"command\",\n",
    "            \"log\",\n",
    "            \"undo\",\n",
    "            \"fit\",\n",
    "            \"method\",\n",
    "            \"disk\",\n",
    "            \"settings\",\n",
    "            \"save\",\n",
    "            \"programming\",\n",
    "            \"object\",\n",
    "            \"code\",\n",
    "            \"script\",\n",
    "            \"language\",\n",
    "            \"scripting\",\n",
    "            \"embedding\",\n",
    "            \"function\",\n",
    "            \"C++\",\n",
    "            \"Fortran\",\n",
    "            \"bug\",\n",
    "            \"installation\",\n",
    "            \"syntax\",\n",
    "            \"interface\",\n",
    "            \"architecture\",\n",
    "            \"prompt\",\n",
    "            \"parameter\",\n",
    "            \"graphics\",\n",
    "            \"comment\",\n",
    "            \"wrapper\",\n",
    "            \"terminal\",\n",
    "            \"plot\",\n",
    "            \"show\",\n",
    "            \"save\",\n",
    "            \"clear\",\n",
    "            \"default\",\n",
    "            \"input\",\n",
    "            \"output\",\n",
    "            \"query\",\n",
    "            \"component\",\n",
    "            \"add\",\n",
    "            \"remove\",\n",
    "            \"framework\",\n",
    "            \"instruction\",\n",
    "            \"pan\",\n",
    "            \"view\",\n",
    "            \"browse\",\n",
    "            \"button\",\n",
    "            \"coordinates\",\n",
    "            \"position\",\n",
    "            \"zoom\",\n",
    "            \"dropdown\",\n",
    "            \"search\",\n",
    "            \"git\",\n",
    "            \"github\"\n",
    "        ],\n",
    "        \"mission_keyword\": [\n",
    "            \"mission\",\n",
    "            \"instruments\",\n",
    "            \"trajectory\",\n",
    "            \"meter\",\n",
    "            \"spacecraft\",\n",
    "            \"project\",\n",
    "            \"objectives\",\n",
    "            \"measurement\",\n",
    "            \"aircraft\",\n",
    "            \"camera\",\n",
    "            \"campaign\",\n",
    "            \"flight\",\n",
    "            \"radar\",\n",
    "            \"test\",\n",
    "            \"mobility\",\n",
    "            \"curiosity\",\n",
    "            \"suspension\",\n",
    "            \"investigate\"\n",
    "        ],\n",
    "        \"training_keyword\": [\"training\", \"education\"]\n",
    "    },\n",
    "    \"model_parameters\": {\n",
    "        \"model\": \"AutoModelForSequenceClassification\",\n",
    "        \"tokenizer\": \"AutoTokenizer\",\n",
    "        \"model_type\": \"nasa-impact/document-classifier\",\n",
    "        \"module_name\": \"transformers\",\n",
    "        \"num_labels\": 6,\n",
    "        \"device\": \"cpu\",\n",
    "        \"optimizer\": \"AdamW\",\n",
    "        \"scheduler\": \"get_linear_schedule_with_warmup\"\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "class Encoder:\n",
    "    def __init__(self, config, data):\n",
    "        \"\"\"\n",
    "        Initializes an Encoder object.\n",
    "        Args:\n",
    "            data: The data to be encoded.\n",
    "            config (dict): A dictionary containing configuration parameters for the encoder.\n",
    "            - \"encoder\" (dict): A sub-dictionary containing encoder-related configuration.\n",
    "                    - \"image_keyword\" (list): The list of keywords associated with image data.\n",
    "                    - \"software_keyword\" (list): The list of keywords associated with software and tools data.\n",
    "                    - \"mission_keyword\" (list): The list of keywords associated with mission and instruments data.\n",
    "                    - \"training_keyword\" (list): The list of keywords associated with training and education data.\n",
    "\n",
    "        \"\"\"\n",
    "        self.image_keyword = config[\"encoder\"][\"image_keyword\"]\n",
    "        self.software_keyword = config[\"encoder\"][\"software_keyword\"]\n",
    "        self.mission_keyword = config[\"encoder\"][\"mission_keyword\"]\n",
    "        self.training_keyword = config[\"encoder\"][\"training_keyword\"]\n",
    "        self.data = data\n",
    "        self.encoded_data = pd.DataFrame()\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, cfg: dict, data):\n",
    "        \"\"\"\n",
    "        Creates an Encoder object from a dictionary and data.\n",
    "\n",
    "        Args:\n",
    "            cfg (dict): A dictionary containing configuration parameters for the encoder.\n",
    "            data (dataFrame): A python dataframe with columns class, links and text response from the url\n",
    "\n",
    "        Returns:\n",
    "            Encoder: An instance of the Encoder class.\n",
    "\n",
    "        \"\"\"\n",
    "        return cls(cfg, data)\n",
    "\n",
    "    def generate_text_slice(self, word_positions, text_whole):\n",
    "        \"\"\"\n",
    "        Generates a text slice based on the positions of specific words in the given text.\n",
    "\n",
    "        Args:\n",
    "            word_positions (dict): A dictionary containing the positions of\n",
    "            specific words in the text.\n",
    "            text_whole (str): The text which needs to be sliced\n",
    "        Returns:\n",
    "            str: The extracted text slice based on the positions of the words.\n",
    "\n",
    "        \"\"\"\n",
    "        # Find the minimum position of the words\n",
    "        min_position = min(min(positions) for positions in word_positions.values())\n",
    "        # Calculate the start and end indices for the text slice\n",
    "        start_index = max(min_position[0] - 50, 0)\n",
    "        end_index = min(start_index + 1000, len(text_whole) - 1)\n",
    "        # Extract the text slice based on the start and end indices\n",
    "        text_slice = text_whole[start_index:end_index]\n",
    "        return text_slice\n",
    "\n",
    "    def extract_text(self, text):\n",
    "        \"\"\"\n",
    "        Extracts a text slice based on the occurrence and positions of specific keywords.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to extract the slice from.\n",
    "\n",
    "        Returns:\n",
    "            str: The extracted text slice based on the positions of the keywords.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        keywords = (\n",
    "            self.image_keyword\n",
    "            + self.software_keyword\n",
    "            + self.mission_keyword\n",
    "            + self.training_keyword\n",
    "        )\n",
    "        software_count, mission_count, image_count, training_count = 0, 0, 0, 0\n",
    "        word_positions = {}\n",
    "        start = -1\n",
    "        for word in text.split():\n",
    "            start = text.find(word, start + 1)\n",
    "            end = start + len(word)\n",
    "            if word in keywords and word not in word_positions:\n",
    "                word_positions[word] = []\n",
    "\n",
    "            if word in self.image_keyword:\n",
    "                image_count = image_count + 1\n",
    "                word_positions[word].append((start, end))\n",
    "\n",
    "            if word in self.software_keyword:\n",
    "                software_count = software_count + 1\n",
    "                word_positions[word].append((start, end))\n",
    "\n",
    "            if word in self.mission_keyword:\n",
    "                mission_count = mission_count + 1\n",
    "                word_positions[word].append((start, end))\n",
    "\n",
    "            if word in self.training_keyword:\n",
    "                training_count = training_count + 1\n",
    "                word_positions[word].append((start, end))\n",
    "\n",
    "        if (\n",
    "            software_count == 0\n",
    "            and mission_count == 0\n",
    "            and image_count == 0\n",
    "            and training_count == 0\n",
    "        ):\n",
    "            mid = int(len(text) / 2)\n",
    "            start_pos = mid - 512\n",
    "            end_pos = mid + 512  # in terms of characters\n",
    "            text_slice = text[start_pos:end_pos]\n",
    "        else:\n",
    "            text_slice = self.generate_text_slice(word_positions, text)\n",
    "        return text_slice\n",
    "\n",
    "    def encoder(self):\n",
    "        \"\"\"\n",
    "        Encodes the data by processing the text, URLs, and classes.\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame: The encoded data with processed text, URLs, and classes.\n",
    "\n",
    "        \"\"\"\n",
    "#         text_list, urls_list, class_list = [], [], []\n",
    "        text_list, class_list = [], []\n",
    "        for _, row in self.data.iterrows():\n",
    "            text = row[\"text\"]\n",
    "            classes = row[\"class\"]\n",
    "            counter = text.split()\n",
    "            if len(counter) <= 400:\n",
    "                text_list.append(text)\n",
    "                class_list.append(classes)\n",
    "            elif len(counter) > 400:\n",
    "                text = self.extract_text(text)\n",
    "                text_list.append(text)\n",
    "                class_list.append(classes)\n",
    "        self.encoded_data[\"text\"] = text_list\n",
    "        self.encoded_data[\"class\"] = class_list\n",
    "        return self.encoded_data\n",
    "\n",
    "    def text_encoder(self, text_blob):\n",
    "        \"\"\"\n",
    "        Encodes the data by processing the text, URLs, and classes.\n",
    "        input: text_blob of string\n",
    "        Returns:\n",
    "            str: encoded blob of string\n",
    "\n",
    "        \"\"\"\n",
    "        counter = text_blob.split()\n",
    "        if len(counter) <= 400:\n",
    "            text = text_blob\n",
    "        elif len(counter) > 400:\n",
    "            text = self.extract_text(text_blob)\n",
    "        return text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DataLoad:\n",
    "    def __init__(self, batch_size=10):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of the class.\n",
    "\n",
    "        Args:\n",
    "            config (dict): Configuration parameters for the class.\n",
    "            batch_size (int): The batch size for data loading. Default is 8.\n",
    "\n",
    "        Attributes:\n",
    "            batch_size (int): The batch size for data loading.\n",
    "            self.inference_dataset: The inferencing dataset\n",
    "        \"\"\"\n",
    "\n",
    "        self.inference_dataset = None\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, cfg: dict):\n",
    "        \"\"\"\n",
    "        Creates an DataLoad object from a dictionary\n",
    "\n",
    "        Args:\n",
    "            cfg (dict): A dictionary containing configuration parameters for the loader\n",
    "\n",
    "        Returns:\n",
    "            DataLoad: An instance of the DataLoad class.\n",
    "\n",
    "        \"\"\"\n",
    "        return cls(batch_size=cfg.get(\"dataload\", {}).get(\"batch_size\"))\n",
    "\n",
    "    def dataset(self, input_ids, attention_masks):\n",
    "        \"\"\"\n",
    "        Converts the input_ids and attention_masks into tensor dataset\n",
    "\n",
    "        Args:\n",
    "            token_results (dictionary): a dictonary containing information about input_ids, attention_masks,\n",
    "            links, and indices for training set, test set and validation set.\n",
    "            input_ids (list): A list of tensors containing input_ids of inference dataset\n",
    "            attention_masks (list): A list of tensors containing attention_masks of the inference dataset\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # Split the samples, and create TensorDatasets for each split.\n",
    "\n",
    "        self.inference_dataset = TensorDataset(input_ids, attention_masks)\n",
    "\n",
    "    def dataloader(self):\n",
    "        \"\"\"\n",
    "        Create data loaders for inference\n",
    "\n",
    "        Returns:\n",
    "            inference_dataloader (DataLoader): Data loader for the inference samples.\n",
    "        \"\"\"\n",
    "\n",
    "        inference_sampler = SequentialSampler(self.inference_dataset)\n",
    "        inference_dataloader = DataLoader(\n",
    "            self.inference_dataset,\n",
    "            sampler=inference_sampler,\n",
    "            batch_size=self.batch_size,\n",
    "        )\n",
    "        return inference_dataloader\n",
    "\n",
    "\n",
    "\n",
    "class ModelBert:\n",
    "    \"\"\"\n",
    "    A class for predicting the correct long form for a given acronym\n",
    "    in a context paragraph using a pre-trained BERT model and tokenizer.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, num_labels=6, device=\"cpu\"):\n",
    "        self.config = config[\"model_parameters\"]\n",
    "        self.num_labels = num_labels\n",
    "        self.device = torch.device(device)\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.state_dict = None\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, cfg: dict):\n",
    "        \"\"\"\n",
    "        Creates an ModelBert object from a dictionary.\n",
    "\n",
    "        Args:\n",
    "            cfg (dict): A dictionary containing configuration parameters for the encoder.\n",
    "            data: The data to be encoded.\n",
    "\n",
    "        Returns:\n",
    "            ModelBert: An instance of the ModelBert class.\n",
    "\n",
    "        \"\"\"\n",
    "        model_parameters = cfg.get(\"model_parameters\")\n",
    "        return cls(\n",
    "            cfg,\n",
    "            num_labels=model_parameters.get(\"num_labels\"),\n",
    "            device=model_parameters.get(\"device\"),\n",
    "        )\n",
    "\n",
    "    def make_model(self):\n",
    "        \"\"\"\n",
    "        Instantiates a pre-trained xlnet model and tokenizer.\n",
    "        Returns:\n",
    "            A tuple containing the model and the tokenizer.\n",
    "        \"\"\"\n",
    "        # Dynamicall import the transformers module\n",
    "        module_name = self.config[\"module_name\"]\n",
    "        transformers = importlib.import_module(module_name)\n",
    "        # Dynamically get the model class from transformers module\n",
    "        model_class = getattr(transformers, self.config[\"model\"])\n",
    "        tokenizer_class = getattr(transformers, self.config[\"model\"])\n",
    "        # Load the tokenizer and model\n",
    "        self.tokenizer = tokenizer_class.from_pretrained(self.config[\"model_type\"])\n",
    "        self.model = model_class.from_pretrained(\n",
    "            self.config[\"model_type\"], num_labels=self.config[\"num_labels\"]\n",
    "        ).to(self.device)\n",
    "        return self.model\n",
    "\n",
    "\n",
    "class TestPredictor:\n",
    "    \"\"\"\n",
    "    Class created for sample predictions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.device = torch.device(config[\"model_parameters\"][\"device\"])\n",
    "        self.dataframe = pd.DataFrame()  # columns=['text','class']\n",
    "        self.config = config\n",
    "        self.classes = self.config[\"classes\"]\n",
    "        transformers = importlib.import_module(\n",
    "            self.config[\"model_parameters\"][\"module_name\"]\n",
    "        )\n",
    "        tokenizer_class = getattr(\n",
    "            transformers, self.config[\"model_parameters\"][\"model\"]\n",
    "        )\n",
    "        # Load the tokenizer and model\n",
    "        self.tokenizer = tokenizer_class.from_pretrained(\n",
    "            self.config[\"model_parameters\"][\"model_type\"]\n",
    "        )\n",
    "        self.pdf_lists = []\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, cfg: dict):\n",
    "        \"\"\"\n",
    "        Creates an Encoder object from a dictionary and data.\n",
    "\n",
    "        Args:\n",
    "            cfg (dict): A dictionary containing configuration parameters for the encoder.\n",
    "\n",
    "        Returns:\n",
    "            Encoder: An instance of the Encoder class.\n",
    "\n",
    "        \"\"\"\n",
    "        return cls(cfg)\n",
    "\n",
    "    def convert_labels_to_class(self, value):\n",
    "        \"\"\"\n",
    "        Converts a label value to its corresponding class/category.\n",
    "\n",
    "        Parameters:\n",
    "            value (int): The label value to be converted.\n",
    "\n",
    "        Returns:\n",
    "            str: The corresponding class/category for the given label value.\n",
    "\n",
    "        \"\"\"\n",
    "        for category, val in self.classes.items():\n",
    "            if val == value:\n",
    "                return category\n",
    "\n",
    "    def process_test_data(self, urls):\n",
    "        \"\"\"\n",
    "        Processes the test data by retrieving content from the provided URL and encoding it.\n",
    "\n",
    "        Parameters:\n",
    "            url (str): The URL of the test data.\n",
    "\n",
    "        Returns:\n",
    "            Union[str, DataFrame]: If the content type is an image, returns \"Image\".\n",
    "                                Otherwise, returns the encoded test data as a DataFrame.\n",
    "\n",
    "        \"\"\"\n",
    "        self.dataframe[\"links\"] = urls\n",
    "        self.dataframe[\"class\"] = [3 for i in urls]  # any random class\n",
    "        processor = Preprocessor.from_dict(self.config, self.dataframe)\n",
    "        (\n",
    "            self.dataframe,\n",
    "            self.pdf_lists,\n",
    "            self.image_lists,\n",
    "        ) = processor.preprocessed_features()\n",
    "        self.dataframe[\"text\"] = self.dataframe[\"soup\"]\n",
    "        encoder = Encoder.from_dict(self.config, self.dataframe)\n",
    "        encoded_data = encoder.encoder()\n",
    "        return encoded_data, self.pdf_lists, self.image_lists\n",
    "\n",
    "    def tokenize_test_data(self, encoded_data):\n",
    "        \"\"\"\n",
    "        Tokenizes the encoded test data using the tokenizer specified in the configuration.\n",
    "\n",
    "        Parameters:\n",
    "            encoded_data (DataFrame): The encoded test data.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tensor, Tensor]: The input IDs and attention masks of the tokenized test data.\n",
    "\n",
    "        \"\"\"\n",
    "        sentence, labels = [], []\n",
    "        for _, row in encoded_data.iterrows():\n",
    "            sentence.append(row[\"text\"])\n",
    "            labels.append(row[\"class\"])\n",
    "        module_name = self.config[\"model_parameters\"][\"module_name\"]\n",
    "        transformers = importlib.import_module(module_name)\n",
    "        # Dynamically get the model class from transformers module\n",
    "        tokenizer_class = getattr(\n",
    "            transformers, self.config[\"model_parameters\"][\"tokenizer\"]\n",
    "        )\n",
    "        tokenizer = tokenizer_class.from_pretrained(\n",
    "            self.config[\"model_parameters\"][\"model_type\"]\n",
    "        )\n",
    "        input_ids, attention_masks = [], []\n",
    "        for sent in sentence:\n",
    "            encoded_dict = tokenizer.encode_plus(\n",
    "                text=sent,\n",
    "                add_special_tokens=True,\n",
    "                truncation=True,\n",
    "                max_length=500,\n",
    "                padding=\"max_length\",\n",
    "                return_attention_mask=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            input_ids.append(encoded_dict[\"input_ids\"])\n",
    "            attention_masks.append(encoded_dict[\"attention_mask\"])\n",
    "        input_ids = torch.cat(input_ids, dim=0)\n",
    "        attention_masks = torch.cat(attention_masks, dim=0)\n",
    "        return input_ids, attention_masks\n",
    "\n",
    "    def tokenize_text_data(self, text_blob):\n",
    "        module_name = self.config[\"model_parameters\"][\"module_name\"]\n",
    "        transformers = importlib.import_module(module_name)\n",
    "        # Dynamically get the model class from transformers module\n",
    "        tokenizer_class = getattr(\n",
    "            transformers, self.config[\"model_parameters\"][\"tokenizer\"]\n",
    "        )\n",
    "        tokenizer = tokenizer_class.from_pretrained(\n",
    "            self.config[\"model_parameters\"][\"model_type\"]\n",
    "        )\n",
    "        inputs = tokenizer(text_blob, return_tensors=\"pt\")\n",
    "        return inputs[\"input_ids\"], inputs[\"attention_mask\"]\n",
    "\n",
    "    def predict_test_data(self, inference_dataloader, loaded_model):\n",
    "        \"\"\"\n",
    "        Predicts the category of the inference data given inference dataloader\n",
    "        Parameters:\n",
    "        inference_dataloader (Dataloader): Dataloader of inference data\n",
    "        Return(s): category: A list of category where each url belongs to\n",
    "        \"\"\"\n",
    "        loaded_model.eval()\n",
    "        predictions = []\n",
    "        for batch in inference_dataloader:\n",
    "            batch = tuple(t.to(self.device) for t in batch)\n",
    "            # Unpack the inputs from our dataloader\n",
    "            b_input_ids, b_input_mask = batch\n",
    "            with torch.no_grad():\n",
    "                # Forward pass, calculate logit predictions.\n",
    "                result = loaded_model(\n",
    "                    b_input_ids,\n",
    "                    token_type_ids=None,\n",
    "                    attention_mask=b_input_mask,\n",
    "                    return_dict=True,\n",
    "                )\n",
    "            logits = result.logits\n",
    "            logits = torch.sigmoid(logits)\n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            # Store predictions and true labels\n",
    "            predictions.extend(logits)\n",
    "\n",
    "        # To get predictions for all urls\n",
    "        preds_position = [np.argmax(arr).tolist() for arr in predictions]\n",
    "        # Respective categories for all the urls\n",
    "        categories = [\n",
    "            self.convert_labels_to_class(position) for position in preds_position\n",
    "        ]\n",
    "        return categories\n",
    "    \n",
    "\n",
    "def batch_predictions(text_lists):\n",
    "    dataframe = pd.DataFrame()\n",
    "    dataframe[\"class\"] = [3 for i in text_lists]  # any random class\n",
    "    dataframe[\"text\"]=text_lists\n",
    "    encoder = Encoder.from_dict(config, dataframe)\n",
    "    encoded_data = encoder.encoder()\n",
    "    if len(encoded_data) > 0:\n",
    "        predictor = TestPredictor.from_dict(config)\n",
    "        input_ids, attention_masks = predictor.tokenize_test_data(\n",
    "            encoded_data\n",
    "        )\n",
    "        loader = DataLoad.from_dict(config)\n",
    "        loader.dataset(input_ids, attention_masks)\n",
    "        inference_dataloader = loader.dataloader()\n",
    "        model=ModelBert.from_dict(config)\n",
    "        loaded_model = model.make_model()\n",
    "        prediction={}\n",
    "        category = predictor.predict_test_data(inference_dataloader, loaded_model)\n",
    "        for enum, each_category in enumerate(category):\n",
    "            prediction[text_lists[enum]] = each_category\n",
    "    return prediction\n",
    "\n",
    "def single_prediction(text_blob:str):\n",
    "    dataframe = pd.DataFrame()\n",
    "    encoder = Encoder.from_dict(config, dataframe)\n",
    "    encoded_data = encoder.text_encoder(text_blob)\n",
    "    predictor = TestPredictor.from_dict(config)\n",
    "    input_ids, attention_masks = predictor.tokenize_text_data(text_blob)\n",
    "    model=ModelBert.from_dict(config)\n",
    "    loaded_model= model.make_model()\n",
    "    with torch.no_grad():\n",
    "        outputs = loaded_model(input_ids=input_ids, attention_mask=attention_masks)\n",
    "    predictions = np.argmax(outputs.logits)\n",
    "    predictor = TestPredictor.from_dict(config)\n",
    "    predictions=predictor.convert_labels_to_class(predictions)\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "053ae103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'This data talks about software and tools needed for developing a package for data visualization': 'Documentation', \"Direct- and Remote-sensing Instruments Direct-sensing instruments, also called contact science instruments, register characteristics of phenomena in their immediate vicinity.The heavy ion counter that flew on Galileo was a direct sensing instrument. It registered the characteristics of ions in the spacecraft's vicinity that actually entered the instrument.\": 'Mission and Instruments'}\n"
     ]
    }
   ],
   "source": [
    "result=batch_predictions([\"This data talks about software and tools needed for developing a package for data visualization\",\n",
    "                          \"Direct- and Remote-sensing Instruments Direct-sensing instruments, also called contact science instruments, register characteristics of phenomena in their immediate vicinity.The heavy ion counter that flew on Galileo was a direct sensing instrument. It registered the characteristics of ions in the spacecraft's vicinity that actually entered the instrument.\"])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f551b3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mission and Instruments\n"
     ]
    }
   ],
   "source": [
    "result=single_prediction(\"Most instruments only receive and process existing light, particles, or other phenomena, and they are said to be passive. Typical of this type would be an imaging instrument viewing a planet that is illuminated by Sunlight, or a magnetometer measuring existing magnetic fields.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04af9d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
