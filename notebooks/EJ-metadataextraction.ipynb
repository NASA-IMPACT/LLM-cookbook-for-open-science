{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beautifulsoup and requests to get content from url\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def get_text_from_html(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "text = get_text_from_html(\"https://lpdaac.usgs.gov/news/release-nasadem-data-products/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Indicators                                        Description\n",
      "0     Food Availability  The NASA Making Earth System Data Records for ...\n",
      "1     Food Availability  The NASA Making Earth System Data Records for ...\n",
      "2     Food Availability  The NASA Making Earth System Data Records for ...\n",
      "3     Food Availability  The NASA Making Earth System Data Records for ...\n",
      "4     Food Availability  The NASA Making Earth System Data Records for ...\n",
      "..                  ...                                                ...\n",
      "254      Urban Flooding  The Global Flood Hazard Frequency and Distribu...\n",
      "255      Urban Flooding  Precipitation data from the GPM and TRMM missi...\n",
      "256      Urban Flooding  The Precipitation Processing System (PPS) evol...\n",
      "257      Urban Flooding  The Low Elevation Coastal Zone (LECZ) Urban-Ru...\n",
      "258  Water Availability  This dataset contains gridded monthly global w...\n",
      "\n",
      "[259 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace 'your_excel_file.xlsx' with the actual file path of your Excel sheet.\n",
    "excel_file_path = \"ej_dataset.xlsx\"\n",
    "\n",
    "# Load the Excel sheet into a DataFrame.\n",
    "df = pd.read_excel(excel_file_path, engine=\"openpyxl\")\n",
    "# Display the extracted information.\n",
    "print(df)\n",
    "#df remove none\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Climate Change -> 0\n",
      "Disasters -> 1\n",
      "Extreme Heat -> 2\n",
      "Food Availability -> 3\n",
      "Health & Air Quality -> 4\n",
      "Human Dimensions -> 5\n",
      "Urban Flooding -> 6\n",
      "Water Availability -> 7\n"
     ]
    }
   ],
   "source": [
    "## encode labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df.Indicators.unique()\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(df['Indicators'])\n",
    "# print the label mapping\n",
    "for index, item in enumerate(label_encoder.classes_):\n",
    "    print(item, '->', index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at nasa-impact/nasa-smd-ibm-v0.1 and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/work/llm-tooling-kit/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:748\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> 748\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    750\u001b[0m     \u001b[38;5;66;03m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[1;32m    751\u001b[0m     \u001b[38;5;66;03m# # at-least2d\u001b[39;00m\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;66;03m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[1;32m    753\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[1;32m    754\u001b[0m     \u001b[38;5;66;03m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[1;32m    755\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor[None, :]\u001b[39;00m\n",
      "File \u001b[0;32m~/work/llm-tooling-kit/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:720\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors.<locals>.as_tensor\u001b[0;34m(value, dtype)\u001b[0m\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(value))\n\u001b[0;32m--> 720\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 111 at dim 1 (got 190)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m label_encoder\u001b[38;5;241m.\u001b[39mtransform(test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIndicators\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Tokenize the descriptions and create dataset objects\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m train_encodings \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDescription\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m test_encodings \u001b[38;5;241m=\u001b[39m tokenizer(\u001b[38;5;28mlist\u001b[39m(test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDescription\u001b[39m\u001b[38;5;124m'\u001b[39m]), return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     45\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m CustomDataset(train_encodings, torch\u001b[38;5;241m.\u001b[39mtensor(train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()))\n",
      "File \u001b[0;32m~/work/llm-tooling-kit/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2798\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2796\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2797\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2798\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2800\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/work/llm-tooling-kit/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2884\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2879\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2880\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2881\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2882\u001b[0m         )\n\u001b[1;32m   2883\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[0;32m-> 2884\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2885\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2886\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2891\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2895\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2896\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2897\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2898\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2901\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2902\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2903\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2904\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m   2905\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   2906\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2922\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2923\u001b[0m     )\n",
      "File \u001b[0;32m~/work/llm-tooling-kit/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3075\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3065\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3066\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3067\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3068\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3072\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3073\u001b[0m )\n\u001b[0;32m-> 3075\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3077\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3081\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3084\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3085\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3086\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3089\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3090\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3091\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3092\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3093\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/llm-tooling-kit/.venv/lib/python3.11/site-packages/transformers/models/roberta/tokenization_roberta_fast.py:270\u001b[0m, in \u001b[0;36mRobertaTokenizerFast._batch_encode_plus\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m is_split_into_words \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_split_into_words\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_prefix_space \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_split_into_words, (\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to instantiate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with add_prefix_space=True \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto use it with pretokenized inputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    268\u001b[0m )\n\u001b[0;32m--> 270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/llm-tooling-kit/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:552\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_ids \u001b[38;5;129;01min\u001b[39;00m sanitized_tokens[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eventual_warn_about_too_long_sequence(input_ids, max_length, verbose)\n\u001b[0;32m--> 552\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatchEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43msanitized_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msanitized_encodings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/llm-tooling-kit/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:223\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    219\u001b[0m     n_sequences \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_sequences\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_sequences \u001b[38;5;241m=\u001b[39m n_sequences\n\u001b[0;32m--> 223\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/llm-tooling-kit/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:764\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverflowing_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    760\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    761\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    762\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    763\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    765\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor, you should probably activate truncation and/or padding with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    766\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpadding=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruncation=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to have batched tensors with the same length. Perhaps your\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    767\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m features (`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    768\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    769\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Create a custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.encodings['input_ids'][idx],\n",
    "            'attention_mask': self.encodings['attention_mask'][idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "\n",
    "# Load your domain-specific encoder model (replace 'model_name' with your model's name)\n",
    "model_name = 'nasa-impact/nasa-smd-ibm-v0.1'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Load your dataset into a DataFrame (replace 'your_data.csv' with your data file)\n",
    "data = df\n",
    "\n",
    "# Split the data into training and testing sets (80% for training, 20% for testing)\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42, stratify=data['Indicators'])\n",
    "\n",
    "# Use LabelEncoder to convert class labels (Indicators) to numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_data['labels'] = label_encoder.fit_transform(train_data['Indicators'])\n",
    "test_data['labels'] = label_encoder.transform(test_data['Indicators'])\n",
    "\n",
    "# Tokenize the descriptions and create dataset objects\n",
    "train_encodings = tokenizer(list(train_data['Description']), return_tensors='pt')\n",
    "test_encodings = tokenizer(list(test_data['Description']), return_tensors='pt')\n",
    "\n",
    "train_dataset = CustomDataset(train_encodings, torch.tensor(train_data['labels'].tolist()))\n",
    "test_dataset = CustomDataset(test_encodings, torch.tensor(test_data['labels'].tolist()))\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    output_dir=\"./output\",\n",
    "    num_train_epochs=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "# Define trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=lambda p: classification_report(p.label_ids, p.predictions.argmax(-1), output_dict=True),\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "results = trainer.evaluate()\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ej_tokenizer/tokenizer_config.json',\n",
       " 'ej_tokenizer/special_tokens_map.json',\n",
       " 'ej_tokenizer/vocab.json',\n",
       " 'ej_tokenizer/merges.txt',\n",
       " 'ej_tokenizer/added_tokens.json',\n",
       " 'ej_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Save the trained model\n",
    "model.save_pretrained('ej_classifier')  # Replace 'your_model_directory' with your desired directory\n",
    "tokenizer.save_pretrained('ej_tokenizer')  # Save the tokenizer as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description: This monthly climatology data set contains a series of land surface parameters simulated from the Noah land-surface model (LSM) for Phase 2 of the North American Land Data Assimilation System (NLDAS-2).  The data are in 1/8th degree grid spacing. The temporal resolution is monthly, ranging from January to December. The NLDAS-2 monthly climatology data are the monthly data averaged over the thirty years (1980 - 2009) of the NLDAS-2 monthly data. The file format is WMO GRIB-1.\n",
      "Predicted Label: Climate Change\n",
      "\n",
      "Description: The NASA Making Earth System Data Records for Use in Research Environments (MEaSUREs) (https://earthdata.nasa.gov/community/community-data-system-programs/measures-projects) Global Food Security-support Analysis Data (GFSAD) data product provides cropland extent data over Southeast and Northeast Asia for nominal year 2015 at 30 meter resolution (GFSAD30SEACE). The monitoring of global cropland extent is critical for policymaking and provides important baseline data that are used in many agricultural cropland studies pertaining to water sustainability and food security. 2013-01-01 to 2016-12-31\n",
      "\n",
      "Predicted Label: Disasters\n",
      "\n",
      "Description: The Terra Moderate Resolution Imaging Spectroradiometer (MODIS) Land Surface Temperature/Emissivity Daily (MOD11A1) Version 6.1 product provides daily per-pixel Land Surface Temperature and Emissivity (LST&E) with 1 kilometer (km) spatial resolution in a 1,200 by 1,200 km grid. The pixel temperature value is derived from the MOD11_L2 swath product. Above 30 degrees latitude, some pixels may have multiple observations where the criteria for clear-sky are met. When this occurs, the pixel value is a result of the average of all qualifying observations. Provided along with the daytime and nighttime surface temperature bands are associated quality control assessments, observation times, view zenith angles, and clear-sky coverages along with bands 31 and 32 emissivities from land cover types.\n",
      "Predicted Label: Climate Change\n",
      "\n",
      "Description: Terrestrial AOD data (3km resolution, merged algorithm)\n",
      "Predicted Label: Disasters\n",
      "\n",
      "Description: The SEDAC Hazards Mapper enables users to visualize data and map layers related to Socioeconomic, Infrastructure, Natural Disasters, and Environment and analyze potential impacts and exposure. The web app applies layers from various sources including SEDAC, NASA LANCE, NASA GIBS, USGS, NOAA, ESRI, and others.\n",
      "Predicted Label: Disasters\n",
      "\n",
      "Description: The Global Flood Total Economic Loss Risk Deciles is a 2.5 minute grid of global flood total economic loss risks. A process of spatially allocating Gross Domestic Product (GDP) based upon the Sachs et al. (2003) methodology is utilized. First the proportional contributions of subnational units to their respective national GDP are determined using sources of various origins. The contribution rates are then applied to published World Bank Development Indicators to determine a GDP value for the subnational unit. Once the national GDP has been spatially stratified into the smallest administrative units available, GDP values for grid cells are derived using Gridded Population of the World, Version 3 (GPWv3) data of population distributions. A per capita contribution value is determined within each subnational unit, and this value is multiplied by the population per grid cell. Once a GDP value has been determined on a per grid cell basis, then the regionally variable loss rate as derived from the historical records of EM-DAT is used to determine the total economic loss risks posed to a grid cell by flood hazards. The final surface does not present absolute values of total economic loss, but rather a relative decile (1-10 with increasing risk) ranking of grid cells based upon the calculated economic loss risks. This data set is the result of collaboration among the Columbia University Center for Hazards and Risk Research (CHRR), International Bank for Reconstruction and Development/The World Bank, and Columbia University Center for International Earth Science Information Network (CIESIN).\n",
      "Predicted Label: Disasters\n",
      "\n",
      "Description: The West Africa Coastal Vulnerability Mapping: Gridded Subset of Sub-national Poverty and Extreme Poverty Prevalence represents the HarvestChoice Subnational Poverty and Extreme Poverty Prevalence data set as a one kilometer raster, and includes values within 200 kilometers of the coast. Poverty levels affect the \"defenselessness\" of populations in the low elevation coastal zone. These data were developed by the Harvest Choice project funded by the Bill and Melinda Gates Foundation. Harvest Choice measured 2005 poverty levels using 2005 purchasing power parity data for two thresholds: $1.25/day and $2/day international poverty lines. The $2/day threshold was selected for this mapping exercise.\n",
      "Predicted Label: Disasters\n",
      "\n",
      "Description: The U.S. Census Grids (Summary File 1), 2010 data set contains grids of demographic and socioeconomic data from the year 2010 in ASCII and GeoTIFF formats. The grids have a resolution of 30 arc-seconds (0.0083 decimal degrees), or approximately 1 square km. The gridded variables are based on census block geography from Census 2010 TIGER/Line Files and census variables (population, households, and housing variables).\n",
      "Predicted Label: Disasters\n",
      "\n",
      "Description: The Version 4.0 Aura Ozone Monitoring Instrument (OMI) Nitrogen Dioxide (NO2) Standard Product (OMNO2) is now available from the NASA Goddard Earth Sciences Data and Information Services Center. The major V4.0 updates include: (1) use of a new daily and OMI ﬁeld of view speciﬁc geometry dependent surface Lambertian Equivalent Reﬂectivity (GLER) product in both NO2 and cloud retrievals; (2) use of improved cloud parameters (eﬀective cloud fraction and cloud optical centroid pressure) from a new cloud algorithm (OMCDO2N) that are retrieved consistently with NO2 using a new algorithm for O2-O2 slant column data and the GLER product for terrain reﬂectivity; (3) use of a more accurate terrain pressure calculated using OMI ground pixel-averaged terrain height and monthly mean GMI terrain pressure; and (4) improved treatment over snow/ice surfaces by using the concept of scene LER and scene pressure. The details can be found in the updated OMNO2 readme document (see Documentation). The OMNO2 product contains slant column NO2 (total amount along the average optical path from the sun into the atmosphere, and then toward the satellite), the total NO2 vertical column density (VCD), the stratospheric and tropospheric VCDs, air mass factors (AMFs), scattering weights for calculation of AMFs, and other ancillary data. \n",
      "Predicted Label: Climate Change\n",
      "\n",
      "Description: The Global Annual PM2.5 Grids from MODIS, MISR and SeaWiFS Aerosol Optical Depth (AOD) with GWR, 1998-2016 consist of annual concentrations (micrograms per cubic meter) of ground-level fine particulate matter (PM2.5), with dust and sea-salt removed. This data set combines AOD retrievals from multiple satellite instruments including the NASA Moderate Resolution Imaging Spectroradiometer (MODIS), Multi-angle Imaging SpectroRadiometer (MISR), and the Sea-Viewing Wide Field-of-View Sensor (SeaWiFS). The GEOS-Chem chemical transport model is used to relate this total column measure of aerosol to near-surface PM2.5 concentration. Geographically Weighted Regression (GWR) is used with global ground-based measurements to predict and adjust for the residual PM2.5 bias per grid cell in the initial satellite-derived values. These estimates are primarily intended to aid in large-scale studies\n",
      "Predicted Label: Climate Change\n",
      "\n",
      "Description: AIRS measures abundances of trace components in the atmosphere including CO. Data are available daily (AIRS3STD), over 8 days (AIRS3ST8), or monthly (AIRS3STM). The instrument measures the amount of CO in the total vertical column profile of the atmosphere (from Earth’s surface to top-of-atmosphere). Data are in HDF format, and can be opened using Panoply.\n",
      "Predicted Label: Disasters\n",
      "\n",
      "Description: Through a combination of airborne radar and ground-based GPS, a research team has developed detailed models of how much land is sinking and rising in southern Louisiana.\n",
      "Predicted Label: Disasters\n",
      "\n",
      "Description: The AIRS Level 1B Near Real Time (NRT) product (AIRIBRAD_NRT_005) differs from the routine product (AIRIBRAD_005) in 2 ways to meet the three hour latency requirements of the Land Atmosphere NRT Capability Earth Observing System (LANCE): (1) The NRT granules are produced without previous or subsequent granules if those granules are not available within 5 minutes, (2) the predictive ephemeris/attitude data are used rather than the definitive ephemeris/attitude. \n",
      "Predicted Label: Disasters\n",
      "\n",
      "Description: The Modern-Era Retrospective analysis for Research and Applications, Version 2 (MERRA-2) provides data beginning in 1980. Due to the amount of historical data available, MERRA-2 data can be used to look for trends and patterns, as well as anomalies. There are several options available: 1-hourly, 3-hourly, 6-hourly. These options provide information on surface skin temperature, the air temperature at 2 m, and the air temperature at 10 m.\n",
      "Predicted Label: Disasters\n",
      "\n",
      "Description: The U.S. Social Vulnerability Index Grids data set contains gridded layers for the overall Centers for Disease Control and Prevention (CDC) Social Vulnerability Index (SVI) using four sub-category themes (Socioeconomic, Household Composition & Disability, Minority Status & Language, and Housing Type & Transportation) based on census tract level inputs from 15 variables for the years 2000, 2010, 2014, 2016, and 2018. SVI values range between 0 and 1 based on their percentile position among all census tracts in the U.S., with 0 representing lowest vulnerability census tracts and 1 representing highest vulnerability census tracts. SEDAC has gridded these vector inputs to create 1 km spatial resolution raster surfaces allowing users to obtain vulnerability metrics for any user-defined area within the U.S. Utilizing inputs from CIESIN's Gridded Population of the World, Version 4, Revision 11 (GPWv4.11), a mask is applied for water, and optionally, for no population. The data are provided in two different projection formats, NAD83 as a U.S. specific standard, and WGS84 as a global standard. The goal of the SVI is to help identify vulnerable communities by ranking them on these inputs across the U.S.\n",
      "Predicted Label: Disasters\n",
      "\n",
      "Description: NASA's Atmospheric Infrared Sounder (AIRS) on NASA's Aqua satellite gathers infrared energy emitted from Earth's surface and atmosphere globally every day. AIRS data are daily, 8-day, and monthly at 1 degree and the Level 3 data products are provided in either the descending (equatorial crossing North to South at 1:30 a.m. local time) or ascending (equatorial crossing South to North at 1:30 p.m. local time) orbit. When you open the file in HDF format (in a program like Panoply or QGIS), you will see an ascending option and a descending option each with SurfAirTemp.\n",
      "Predicted Label: Disasters\n",
      "\n",
      "Description: The OMPS-NPP L2 NM Aerosol Index swath orbital product provides aerosol index values from the Ozone Mapping and Profiling Suite (OMPS) Nadir-Mapper (NM) instrument on the Suomi-NPP satellite. This is now the official NASA aerosol index product, replacing the aerosol index found in the OMPS-NPP L2 NM Total Ozone product. The aerosol index is derived from normalized radiances using 2 wavelength pairs at 340 and 378.5 nm. Additionally, this data product contains measurements of normalized radiances, reflectivity, cloud fraction, reflectivity, and other ancillary variables. Each granule contains data from the daylight portion of each orbit measured for a full day. Spatial coverage is global (-90 to 90 degrees latitude), and there are about 14.5 orbits per day, each has typically 400 swaths. The swath width of the NM is about 2800 km with 36 scenes, or pixels, with a footprint size of 50 km x 50 km at nadir.\n",
      "Predicted Label: Climate Change\n",
      "\n",
      "Description: The Global Flood Proportional Economic Loss Risk Deciles is a 2.5 minute grid of flood hazard economic loss as proportions of Gross Domestic Product (GDP) per analytical unit. Estimates of GDP at risk are based on regional economic loss rates derived from historical records of the Emergency Events Database (EM-DAT). Loss rates are weighted by the hazard's frequency and distribution. The methodology of Sachs et al. (2003) is followed to determine baseline estimates of GDP per grid cell. To better reflect the confidence surrounding the data and procedures, the range of proportionalities is classified into deciles, 10 class of an approximately equal number of grid cells of increasing risk. This data set is the result of collaboration among the Columbia University Center for Hazards and Risk Research (CHRR), International Bank for Reconstruction and Development/The World Bank, and Columbia University Center for International Earth Science Information Network (CIESIN).\n",
      "Predicted Label: Disasters\n",
      "\n",
      "Description: This dataset contains along track Sea Surface Height Anomalies (SSHA) from the TOPEX/Poseidon, Jason-1, OSTM/Jason-2, and Jason-3 missions geo-referenced to a mean reference orbit. Altimeter data from the multi-mission Geophysical Data Records (GDRs) have been interpolated to a common reference orbit with biases and cross-calibrations applied so that the derived SSHA are consistent between satellites to form a single homogeneous climate data record. Version 5.0 updates include improved Precise Orbit Determination (POD) with GSFC std2006 standards, and the application of internal tides.\n",
      "Predicted Label: Climate Change\n",
      "\n",
      "Description: The Global Flood Total Economic Loss Risk Deciles is a 2.5 minute grid of global flood total economic loss risks. A process of spatially allocating Gross Domestic Product (GDP) based upon the Sachs et al. (2003) methodology is utilized. First the proportional contributions of subnational units to their respective national GDP are determined using sources of various origins. The contribution rates are then applied to published World Bank Development Indicators to determine a GDP value for the subnational unit. Once the national GDP has been spatially stratified into the smallest administrative units available, GDP values for grid cells are derived using Gridded Population of the World, Version 3 (GPWv3) data of population distributions. A per capita contribution value is determined within each subnational unit, and this value is multiplied by the population per grid cell. Once a GDP value has been determined on a per grid cell basis, then the regionally variable loss rate as derived from the historical records of EM-DAT is used to determine the total economic loss risks posed to a grid cell by flood hazards. The final surface does not present absolute values of total economic loss, but rather a relative decile (1-10 with increasing risk) ranking of grid cells based upon the calculated economic loss risks. This data set is the result of collaboration among the Columbia University Center for Hazards and Risk Research (CHRR), International Bank for Reconstruction and Development/The World Bank, and Columbia University Center for International Earth Science Information Network (CIESIN).\n",
      "Predicted Label: Disasters\n",
      "\n",
      "Description: (daily global data coverage)\n",
      "Predicted Label: Disasters\n",
      "\n",
      "Description: The Urban Heat Island (UHI) effect represents the relatively higher temperatures found in urban areas compared to surrounding rural areas owing to higher proportions of impervious surfaces and the release of waste heat from vehicles and heating and cooling systems. Paved surfaces and built structures tend to absorb shortwave radiation from the sun and release long-wave radiation after a lag of a few hours. The Global Urban Heat Island (UHI) Data Set, 2013, estimates the land surface temperature within urban areas in degrees Celsius (average summer daytime maximum and average summer nighttime minimum) as well as the difference between those temperatures and the temperatures in surrounding rural areas, defined as a 10km buffer around the urban extent. Urban extents are from SEDAC?s Global Rural-Urban Mapping Project, Version 1 (GRUMPv1), and land surface temperatures are from SEDAC?s Global Summer Land Surface Temperature (LST) Grids, 2013, which are derived from the Aqua Level-3 Moderate Resolution Imaging Spectroradiometer (MODIS) Version 5 global daytime and nighttime Land Surface Temperature (LST) 8-day composite data (MYD11A2). For most regions, the UHI data set provides the average daytime maximum (1:30 p.m. overpass) and average nighttime minimum (1:30 a.m. overpass) temperatures in urban and rural areas, and the urban-rural temperature differences, derived from LST data representing a 40-day time-span during July-August (Julian days 185-224) in the northern hemisphere and January-February (Julian days 001-040) in the southern hemisphere. LST grid cells with missing values resulting from high cloud cover in tropical regions were filled with daytime maximum and nighttime minimum LST values from April-May 2013 in the northern hemisphere and December 2013-January 2014 in the southern hemisphere, where available. Some data gaps remain in areas where data were insufficient (e.g., Central Africa).\n",
      "Predicted Label: Disasters\n",
      "\n",
      "Description: The Daily Moderate Resolution Imaging Spectroradiometer (MODIS) (Bidirectional Reflectance Distribution Function and Albedo (BRDF/Albedo) 30 arc second, Global Gap-Filled, Snow-Free, (MCD43GF) Version 6 is derived from the 30 arc second Climate Modeling Grid (CMG) MCD43D Version 6 product suite, with additional processing to provide a gap-filled, snow-free product.\n",
      "Predicted Label: Disasters\n",
      "\n",
      "Description: The Global Gridded Geographically Based Economic Data (G-Econ), Version 4 contains derived one degree grid cells of Gross Domestic Product (GDP) data in Grid and ASCII formats for both Market Exchange Rate (MER) and Purchasing Power Parity (PPP) for the years 1990, 1995, 2000 and 2005. MER is the exchange rate between local and U.S. dollar currencies for a given time period established by the market. PPP is the exchange rate between a country's currency and U.S. dollars adjusted to reflect the actual cost in U.S. dollars of purchasing a standardized market basket of goods in that country using the country's currency. The original data from the G-Econ Project at Yale University is also available in tabular format and includes latitude and longitude geographic coordinates of the grid cells, area of grid cells, as well as country names, distance to coast, elevation, vegetation, population, precipitation and temperature.\n",
      "Predicted Label: Disasters\n",
      "\n",
      "Description: The Global Reservoir and Dam Database, Version 1, Revision 01 (v1.01) contains 6,862 records of reservoirs and their associated dams with a cumulative storage capacity of 6,197 cubic km. The dams were geospatially referenced and assigned to polygons depicting reservoir outlines at high spatial resolution. Dams have multiple attributes, such as name of the dam and impounded river, primary use, nearest city, height, area and volume of reservoir, and year of construction (or commissioning). While the main focus was to include all dams associated with reservoirs that have a storage capacity of more than 0.1 cubic kilometers, many smaller dams and reservoirs were added where data were available. The data were compiled by Lehner et al. (2011) and are distributed by the Global Water System Project (GWSP) and by the Columbia University Center for International Earth Science Information Network (CIESIN). For details please refer to the Technical Documentation which is provided with the data.\n",
      "Predicted Label: Disasters\n",
      "\n",
      "Description: This dataset contains along track Sea Surface Height Anomalies (SSHA) from the TOPEX/Poseidon, Jason-1, OSTM/Jason-2, and Jason-3 missions geo-referenced to a mean reference orbit. Altimeter data from the multi-mission Geophysical Data Records (GDRs) have been interpolated to a common reference orbit with biases and cross-calibrations applied so that the derived SSHA are consistent between satellites to form a single homogeneous climate data record. Version 5.0 updates include improved Precise Orbit Determination (POD) with GSFC std2006 standards, and the application of internal tides.\n",
      "Predicted Label: Climate Change\n",
      "\n",
      "Description: The SEDAC Hazards Mapper enables users to visualize data and map layers related to Socioeconomic, Infrastructure, Natural Disasters, and Environment and analyze potential impacts and exposure. The web app applies layers from various sources including SEDAC, NASA LANCE, NASA GIBS, USGS, NOAA, ESRI, and others.\n",
      "Predicted Label: Disasters\n",
      "\n",
      "Description: Copernicus Sentinel-5P tropospheric ozone data products are retrieved by the convective-cloud-differential (CCD) algorithm to derive the tropospheric ozone columns and by the cloud slicing algorithm (CSA) to derive mean upper tropospheric ozone volume mixing ratios above the clouds. The S5P_TROPOZ_CCD algorithm uses TROPOMI Level-2 ozone column measurements and the cloud parameters provided by the S5P_CLOUD_OCRA and S5P_CLOUD_ROCINN, the average values of the tropospheric ozone columns below 270 hpa can be determined. The S5P_TROPOZ_CSA algorithm uses the correlation between could top pressure and the ozone column above the cloud. The retrieval depends on the amount of measurements with a high cloud cover.\n",
      "Predicted Label: Climate Change\n",
      "\n",
      "Description: The Georeferenced U.S. County-Level Population Projections, Total and by Sex, Race and Age, Based on the SSPs, 2020-2100 consists of county-level population projection scenarios of total population, and by age, sex, and race in five-year intervals for all U.S. counties for the period 2020 - 2100. These data have numerous potential uses and can serve as inputs for addressing questions involving sub-national demographic change in the United States in the near, middle- and long-term.\n",
      "Predicted Label: Disasters\n",
      "\n",
      "Description: The Global Multihazard Total Economic Loss Risk Deciles is a 2.5 minute grid of global multihazard total economic loss risks. First, for each of the considered hazards (cyclones, droughts, earthquakes, floods, landslides, and volcanoes), subnational distributions of Gross Domestic Product (GDP) are computed using a methodology developed from Sachs et al. (2003). Where applicable, the contributions of subnational units to national GDP estimates, the contribution ratio, are determined using data of varied origin. World Bank Development Indicators are substituted for GDP estimates of varied origin and the subnational GDP is estimated using the fore mentioned contribution ratios. A subnational, per capita GDP is derived and a final GDP estimate per grid cell is made based on grid cell population density. A raw, total economic loss is computed per grid cell using a regional economic loss rate derived from EM-DAT records. To more accurately reflect the confidence surrounding the economic loss estimate, the range of losses are classified into deciles, 10 classes of an approximately equal number of grid cells. A multihazard index is generated by summing the top three deciles of the individual hazards. This data set is the result of collaboration among the Columbia University Center for Hazards and Risk Research (CHRR), International Bank for Reconstruction and Development/The World Bank, and Columbia University Center for International Earth Science Information Network (CIESIN).\n",
      "Predicted Label: Disasters\n",
      "\n",
      "Description: The Global Cyclone Proportional Economic Loss Risk Deciles is a 2.5 minute grid of cyclone hazard economic loss as proportions of Gross Domestic Product (GDP) per analytical unit. Estimates of GDP at risk are based on regional economic loss rates derived from historical records of the Emergency Events Database (EM-DAT). Loss rates are weighted by the hazard's frequency and distribution. The methodology of Sachs et al. (2003) is followed to determine baseline estimates of GDP per grid cell. To better reflect the confidence surrounding the data and procedures, the range of proportionalities is classified into deciles, 10 class of an approximately equal number of grid cells of increasing risk. This data set is the result of collaboration among the Columbia University Center for Hazards and Risk Research (CHRR), International Bank for Reconstruction and Development/The World Bank, and Columbia University Center for International Earth Science Information Network (CIESIN).\n",
      "Predicted Label: Disasters\n",
      "\n",
      "Description: The Global Multihazard Proportional Economic Loss Risks is a 2.5 minute grid of a multihazard-based economic loss risk as a proportion of the economic productivity of the analytical unit, the grid cell. Representation of multihazard risk is not based on a multihazard index but rather on combinations of hazard risk categories, drought, seismic, and hydro. The drought category includes drought only. The seismic category consists of earthquake and volcano hazards. Cyclones, floods, and landslides are included in the hydro category. For each of the six hazards considered, a binary risk surface is constructed utilizing the three most-at-risk deciles of each hazard's global proportional economic loss risks data set (deciles 8-10). Each of the category risk surfaces are constructed by adding all the relevant hazard high-risk surfaces. These categorical risk surfaces are reclassified into binary high-risk surfaces. The combination of the category risk values forms a three digit identifier for determining those locations that are at higher-risk from multihazards. This data set is the result of collaboration among the Columbia University Center for Hazards and Risk Research (CHRR), International Bank for Reconstruction and Development/The World Bank, and Columbia University Center for International Earth Science Information Network (CIESIN).\n",
      "Predicted Label: Disasters\n",
      "\n",
      "Description: The Global Flood Proportional Economic Loss Risk Deciles is a 2.5 minute grid of flood hazard economic loss as proportions of Gross Domestic Product (GDP) per analytical unit. Estimates of GDP at risk are based on regional economic loss rates derived from historical records of the Emergency Events Database (EM-DAT). Loss rates are weighted by the hazard's frequency and distribution. The methodology of Sachs et al. (2003) is followed to determine baseline estimates of GDP per grid cell. To better reflect the confidence surrounding the data and procedures, the range of proportionalities is classified into deciles, 10 class of an approximately equal number of grid cells of increasing risk. This data set is the result of collaboration among the Columbia University Center for Hazards and Risk Research (CHRR), International Bank for Reconstruction and Development/The World Bank, and Columbia University Center for International Earth Science Information Network (CIESIN).\n",
      "Predicted Label: Disasters\n",
      "\n",
      "Description: The West Africa Coastal Vulnerability Mapping: Gridded Subset of Sub-national Poverty and Extreme Poverty Prevalence represents the HarvestChoice Subnational Poverty and Extreme Poverty Prevalence data set as a one kilometer raster, and includes values within 200 kilometers of the coast. Poverty levels affect the \"defenselessness\" of populations in the low elevation coastal zone. These data were developed by the Harvest Choice project funded by the Bill and Melinda Gates Foundation. Harvest Choice measured 2005 poverty levels using 2005 purchasing power parity data for two thresholds: $1.25/day and $2/day international poverty lines. The $2/day threshold was selected for this mapping exercise.\n",
      "Predicted Label: Disasters\n",
      "\n",
      "Description: The INFORM Global Risk Index 2019 Mid Year, v0.3.7 data set identifies the countries at a high risk of humanitarian crisis that are more likely to require international assistance. The INFORM Global Risk Index (GRI) model is based on risk concepts published in the scientific literature and envisages three dimensions of risk: Hazard & Exposure, Vulnerability, and Lack of Coping Capacity. The INFORM GRI model is split into different levels to provide a quick overview of the underlying factors leading to humanitarian risk. The INFORM GRI model supports a proactive crisis management framework, and will be helpful for an objective allocation of resources for disaster management, as well as for coordinated actions focused on anticipating, mitigating, and preparing for humanitarian emergencies. Only the two main sections, Vulnerability and Lack of Coping Capacity, not the Hazard & Exposure section, were used in the IPCC AR6.\n",
      "Predicted Label: Disasters\n",
      "\n",
      "Description: The Flood Dashboard brings together multiple NASA soil moisture and flood products with products from the National Weather Service and USGS to give a more complete picture of potential flooding in the United States. Includes data from Soil Moisture Map, USGS Stream Gauges, and MODIS Flood Maps.\n",
      "Predicted Label: Disasters\n",
      "\n",
      "Description: The TROPOspheric Monitoring Instrument (TROPOMI) aboard Sentinel 5, is an ESA Mission. ESA's TROPOMI NO2 provides additional information on this level 2 data product. It is important to note that, because of the very small numbers in tropospheric vertical column of NO2, you will need to change the scaling factor in Panoply (see image from June 2018 to right). Data are in NetCDF format, and can be opened using Panoply.\n",
      "Predicted Label: Disasters\n",
      "\n",
      "Description: The FLDAS Global model (McNally et al. 2017) is a custom instance of the NASA Land Information System (LIS; http://lis.gsfc.nasa.gov/) that has been adapted to work with domains, data streams, and monitoring and forecast requirements associated with food security assessment in data-sparse, developing country settings. Adopting LIS allows FEWS NET to leverage existing land surface models and generate ensembles of soil moisture, ET, and other variables based on multiple meteorological inputs or land surface models. The goal of the FLDAS project is to achieve more effective use of limited available hydroclimatic observations and is designed to be adopted for routine use for FEWS NET decision support.\n",
      "Predicted Label: Disasters\n",
      "\n",
      "Description: The Natural Resource Protection and Child Health Indicators, 2021 Release, is produced in support of the U.S. Millennium Challenge Corporation (MCC) as selection criteria for funding eligibility. The Natural Resource Protection Indicator (NRPI) and Child Health Indicator (CHI) are based on proximity-to-target scores ranging from 0 to 100 (at target). The NRPI covers 135 countries and is calculated based on the weighted average percentage of biomes under protected status.\n",
      "Predicted Label: Disasters\n",
      "\n",
      "Description: The Low Elevation Coastal Zone (LECZ) Urban-Rural Population and Land Area Estimates, Version 3 data set contains land areas with urban, quasi-urban, rural, and total populations (counts) within the LECZ for 234 countries and other recognized territories for the years 1990, 2000, and 2015. This data set updates initial estimates for the LECZ population by drawing on a newer collection of input data, and provides a range of estimates for at-risk population and land area. Constructing accurate estimates requires high-quality and methodologically consistent input data, and the LECZv3 evaluates multiple data sources for population totals, digital elevation model, and spatially-delimited urban classifications. Users can find the paper \"Estimating Population and Urban Areas at Risk of Coastal Hazards, 1990-2015: How data choices matter\" (MacManus, et al. 2021) in order to evaluate selected inputs for modeling Low Elevation Coastal Zones. According to the paper, the following are considered core data sets for the purposes of LECZv3 estimates: Multi-Error-Removed Improved-Terrain Digital Elevation Model (MERIT-DEM), Global Human Settlement (GHSL) Population Grid R2019 and Degree of Urbanization Settlement Model Grid R2019a v2, and the Gridded Population of the World, Version 4 (GPWv4), Revision 11. This data set is produced by the Columbia University Center for International Earth Science Information Network (CIESIN) and the City University of New York (CUNY) Institute for Demographic Research (CIDR).\n",
      "Predicted Label: Disasters\n",
      "\n",
      "Description: Facebook collaborates with SEDAC on data; high resolution and updated frequently\n",
      "Predicted Label: Disasters\n",
      "\n",
      "Description: The Precipitation Processing System (PPS) evolved from the Tropical Rainfall Measuring Mission (TRMM) Science Data and Information System (TSDIS). The purpose of the PPS is to process, analyze and archive data from the Global Precipitation Measurement (GPM) mission, partner satellites and the TRMM mission. The PPS also supports TRMM by providing validation products from TRMM ground radar sites.  All GPM, TRMM and Partner public data products are available to the science community and the general public from the TRMM/GPM FTPS and HTTPS Data Archives.\n",
      "Predicted Label: Disasters\n",
      "\n",
      "Description: The MLS Nitric Acid (HNO3) Mixing Ratio at 46hPa layer indicates nitric acid levels at the vertical atmospheric pressure level of 46hPa, and is measured in parts per billion by volume (ppbv). It is derived from the MLS Nitric Acid (ML2HNO3_NRT) MLS/Aura NRT L2 HNO3 Mixing Ratio\n",
      "Predicted Label: Disasters\n",
      "\n",
      "Description: Within the viewer, select the parameter or field of interest, the area of interest, and then indicate the forecast time and the forecast lead hour. Animate shows the forecast for the given parameter over the time period indicated. Note that it may take time to load the images to animate. For those variables near the surface, make sure to select 850 as your level.\n",
      "Predicted Label: Disasters\n",
      "\n",
      "Description: This dataset provides daily-averaged global mean sea level from the ECCO Version 4 Release 4 (V4r4) ocean and sea-ice state estimate. Estimating the Circulation and Climate of the Ocean (ECCO) ocean and sea-ice state estimates are dynamically and kinematically-consistent reconstructions of the three-dimensional time-evolving ocean, sea-ice, and surface atmospheric states. ECCO V4r4 is a free-running solution of the 1-degree global configuration of the MIT general circulation model (MITgcm) that has been fit to observations in a least-squares sense.\n",
      "Predicted Label: Climate Change\n",
      "\n",
      "Description: ESA TROPOMI O3 provides additional information on this level 2 data product. Data are in NetCDF format, and can be opened using Panoply.\n",
      "Predicted Label: Disasters\n",
      "\n",
      "Description: The AERONET Data Display Interface allows users to find and download ground-based AOD (Aerosol Optical Depth, quantity of light removed from a beam by scattering or absorbing during its path through a medium and is a unitless measure) data for locations all across the world. Data are available from years 1993-2022, dependent on location.\n",
      "Predicted Label: Disasters\n",
      "\n",
      "Description: The ARIA Project, a joint effort of the California Institute of Technology and NASA's Jet Propulsion Laboratory, is developing the infrastructure to generate imaging products in near real-time that can improve situational awareness for disaster response.\n",
      "Predicted Label: Disasters\n",
      "\n",
      "Description: The VIIRS Plus DMSP Change in Lights (VIIRS+DMSP dLIGHT) data set fuses nighttime lights imagery from the U.S. Air Force Defense Meteorological Satellite Program (DMSP) Operational Linescan System (OLS) with a stable night light composite from the next generation Suomi National Polar-orbiting Partnership (NPP) Visible Infrared Imaging Radiometer Suite (VIIRS) Day-Night Band to map the spatial distribution and temporal evolution of global nighttime lights between 1992 and 2015. The product visualizes changes in both brightness and extent of nocturnal low lights over two decades while minimizing the spatial overextent (overglow) and bright saturation that compromise the DMSP-OLS composites.\n",
      "Predicted Label: Disasters\n",
      "\n",
      "Description: FEWS NET, the Famine Early Warning Systems Network, is a leading provider of early warning and analysis on acute food insecurity around the world. Created in 1985 by the United States Agency for International Development (USAID) in response to devastating famines in East and West Africa, FEWS NET provides unbiased, evidence-based analysis to governments and relief agencies who plan for and respond to humanitarian crises. FEWS NET analyses support resilience and development programming as well. FEWS NET analysts and specialists work with scientists, government ministries, international agencies, and NGOs to track and publicly report on conditions in the world’s most food-insecure countries.\n",
      "Predicted Label: Disasters\n",
      "\n",
      "Description: This dataset contains the Global Mean Sea Level (GMSL) trend generated from the Integrated Multi-Mission Ocean Altimeter Data for Climate Research Version 5.1. The GMSL trend is a 1-dimensional time series of globally averaged Sea Surface Height Anomalies (SSHA) from TOPEX/Poseidon, Jason-1, OSTM/Jason-2, and Jason-3 that covers September 1992 to present with a lag of up to 4 months. The data are reported as variations relative to a 20-year TOPEX/Jason collinear mean. Bias adjustments and cross-calibrations were applied to ensure SSHA data are consistent across the missions; Glacial Isostatic Adjustment (GIA) was also applied. The data are available as a table in ASCII format. Changes between the version 4.2 and version 5.x releases are described in detail in the user handbook.\n",
      "Predicted Label: Climate Change\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Load the saved model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained('ej_classifier')  # Load from the directory where you saved it\n",
    "tokenizer = AutoTokenizer.from_pretrained('ej_tokenizer')\n",
    "\n",
    "# Get predictions\n",
    "with torch.no_grad():\n",
    "    outputs = model(**test_encodings)\n",
    "\n",
    "# Extract predicted class labels\n",
    "predicted_labels = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "predicted_class_labels = label_encoder.inverse_transform(predicted_labels.numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Climate Change, True Label: Extreme Heat\n",
      "Prediction: Disasters, True Label: Food Availability\n",
      "Prediction: Climate Change, True Label: Extreme Heat\n",
      "Prediction: Disasters, True Label: Health & Air Quality\n",
      "Prediction: Disasters, True Label: Disasters\n",
      "Prediction: Disasters, True Label: Human Dimensions\n",
      "Prediction: Disasters, True Label: Human Dimensions\n",
      "Prediction: Disasters, True Label: Human Dimensions\n",
      "Prediction: Climate Change, True Label: Health & Air Quality\n",
      "Prediction: Climate Change, True Label: Health & Air Quality\n",
      "Prediction: Disasters, True Label: Health & Air Quality\n",
      "Prediction: Disasters, True Label: Urban Flooding\n",
      "Prediction: Disasters, True Label: Health & Air Quality\n",
      "Prediction: Disasters, True Label: Extreme Heat\n",
      "Prediction: Disasters, True Label: Human Dimensions\n",
      "Prediction: Disasters, True Label: Extreme Heat\n",
      "Prediction: Climate Change, True Label: Health & Air Quality\n",
      "Prediction: Disasters, True Label: Human Dimensions\n",
      "Prediction: Climate Change, True Label: Climate Change\n",
      "Prediction: Disasters, True Label: Human Dimensions\n",
      "Prediction: Disasters, True Label: Health & Air Quality\n",
      "Prediction: Disasters, True Label: Extreme Heat\n",
      "Prediction: Disasters, True Label: Health & Air Quality\n",
      "Prediction: Disasters, True Label: Human Dimensions\n",
      "Prediction: Disasters, True Label: Water Availability\n",
      "Prediction: Climate Change, True Label: Climate Change\n",
      "Prediction: Disasters, True Label: Disasters\n",
      "Prediction: Climate Change, True Label: Health & Air Quality\n",
      "Prediction: Disasters, True Label: Human Dimensions\n",
      "Prediction: Disasters, True Label: Human Dimensions\n",
      "Prediction: Disasters, True Label: Human Dimensions\n",
      "Prediction: Disasters, True Label: Human Dimensions\n",
      "Prediction: Disasters, True Label: Human Dimensions\n",
      "Prediction: Disasters, True Label: Human Dimensions\n",
      "Prediction: Disasters, True Label: Human Dimensions\n",
      "Prediction: Disasters, True Label: Urban Flooding\n",
      "Prediction: Disasters, True Label: Health & Air Quality\n",
      "Prediction: Disasters, True Label: Food Availability\n",
      "Prediction: Disasters, True Label: Human Dimensions\n",
      "Prediction: Disasters, True Label: Urban Flooding\n",
      "Prediction: Disasters, True Label: Human Dimensions\n",
      "Prediction: Disasters, True Label: Urban Flooding\n",
      "Prediction: Disasters, True Label: Health & Air Quality\n",
      "Prediction: Disasters, True Label: Extreme Heat\n",
      "Prediction: Climate Change, True Label: Climate Change\n",
      "Prediction: Disasters, True Label: Health & Air Quality\n",
      "Prediction: Disasters, True Label: Health & Air Quality\n",
      "Prediction: Disasters, True Label: Disasters\n",
      "Prediction: Disasters, True Label: Human Dimensions\n",
      "Prediction: Disasters, True Label: Food Availability\n",
      "Prediction: Climate Change, True Label: Climate Change\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
