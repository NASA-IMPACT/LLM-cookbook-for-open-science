{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ceib8WYZHGF"
      },
      "source": [
        "## Chat with mDGF document \n",
        "\n",
        "This notebook loads the Modern Data Governance Framework (MDGF) document and uses generative models to answer user questions about the document. The notebook also allows creation of governance document based on user input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lCeyGy6NZHGG"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# update or install the necessary libraries\n",
        "!pip install --upgrade langchain-openai\n",
        "!pip install --upgrade langchain\n",
        "!pip install --upgrade python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ARDA5TyZHGG",
        "outputId": "b496f8d0-fa8e-4338-f025-835fbd40f2d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ".env file loaded correctly: True\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "print(f\".env file loaded correctly: {load_dotenv()}\")\n",
        "\n",
        "from langchain.chat_models import AzureChatOpenAI\n",
        "#  from langchain_openai import ChatOpenAI  ## use this if you are using the openai chat model\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
        "from langchain.callbacks import get_openai_callback\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
        "from langchain.callbacks import get_openai_callback\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    MessagesPlaceholder\n",
        ")\n",
        "from langchain.schema import (\n",
        "    SystemMessage,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load The MDGF Document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# read the document\n",
        "with open(\"../data/mdgf_document.txt\", \"r\") as file:\n",
        "    mdgf_document = file.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Combining Prompt Patterns\n",
        "- the Persona prompt pattern\n",
        "  - Asking the model to assume persona of expert in scientific data dovernance\n",
        "- Recipe prompt \n",
        "  - Providing the model steps to follow to generate the governance document\n",
        "- Output Automator prompt \n",
        "  - Asking the model to generate a script to automate and provide output in specific format: in this case - the headings required to generate the governance document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hH2VqbsCZHGH",
        "outputId": "aa72381e-7540-481f-d64e-2ce2eb21c088"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/mramasub/work/workshop-usecases-llm/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.azure_openai.AzureChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import AzureChatOpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "import pprint\n",
        "import json\n",
        "\n",
        "MDGF_PROMPT = f\"\"\"\n",
        "You are an expert in scientific data governance and management and you will assist the users by answering questions and creating documents. Use only the content in the Modern Data Governance Framework (MDGF) reference text after the delimiter for your answers. If a questions falls outside the reference text, then respond, “This is out of scope for me to answer”\n",
        "\n",
        "Your responsibilities are two::\n",
        "\n",
        "First - Answering Questions:\n",
        "You will be asked questions. Answer the question only using the reference text provided.\n",
        "Apart from Answering the question, Cite the passages from the document used to answer the question, prefixing it with citation.\n",
        "For Any Requirement, you should also provide the corresponding procedure.\n",
        "If you cannot find an answer in the reference text, then respond, “I could not find the answer”\n",
        "\n",
        "Second - Creating Documents:\n",
        "\n",
        "When asked by a user to create either a requirements document or a procedure plan based on the reference text. Assist the user by asking a series of questions to capture their project needs.\n",
        "\n",
        "Step 1: Identify the entity in the user’s project. Respond with: “Sure, I will be happy to help. First tell me the core entity or asset in that you will be managing\n",
        "\n",
        "Data \n",
        "Metadata\n",
        "Digital content \n",
        "Code\n",
        "Software”\n",
        "\n",
        "Step 2: Identify governance activity in the user’s project. Respond with: “Tell me about the governance activity need in your project\n",
        "\n",
        "Planning and Design\n",
        "Monitoring\n",
        "Generation/Curation\n",
        "Sharing\n",
        "Use/Reuse\n",
        "Preservation”\n",
        "\n",
        "Step 3: Identify the user's need for the Type of document. Respond with: “Are you seeking Requirements or Procedures for your project?\n",
        "\n",
        "Requirements\n",
        "Procedures”\n",
        "\n",
        "Finally, Respond with:\n",
        "\"Here are the headings for the Requirements document:\n",
        "A.1.1.1, A.1.2.1, ...\" \n",
        "You should provide only the headings (A.1.1.1, A.1.2.1, ...) provided in the DGF documents. You should never provide any additional information. Do NOT use placeholder text or ... or anything similar in the response.\n",
        "\n",
        "\n",
        "Here is the reference DGF document:\n",
        "{mdgf_document} \n",
        "\"\"\"\n",
        "\n",
        "# llm = ChatOpenAI(\n",
        "#     temperature=0.5,\n",
        "# \topenai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
        "# \tmodel_name=\"gpt-4-turbo-preview\"\n",
        "# )\n",
        "llm = AzureChatOpenAI(\n",
        "    temperature=0.5,\n",
        "\tmodel_name=\"gpt-4-turbo-preview\"\n",
        ")\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        SystemMessage(\n",
        "            content=MDGF_PROMPT,\n",
        "        ),  # The persistent system prompt\n",
        "        MessagesPlaceholder(\n",
        "            variable_name=\"history\"\n",
        "        ),  # Where the memory will be stored.\n",
        "        HumanMessagePromptTemplate.from_template(\n",
        "            \"{input}\"\n",
        "        ),  # Where the human input will injected\n",
        "    ]\n",
        ")\n",
        "\n",
        "def ask(chain, query, track_token=True):\n",
        "    with get_openai_callback() as cb:\n",
        "        result = chain.invoke(input=query)\n",
        "        if track_token:\n",
        "            print(f'Total tokens: {cb.total_tokens}')\n",
        "            print(f'Requests: {cb.successful_requests}')\n",
        "    print(result['response'])\n",
        "    return result['response']\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    prompt=prompt,\n",
        "    llm=llm,\n",
        "    verbose=False,\n",
        "    memory=ConversationBufferMemory(ai_prefix=\"AI Assistant\", memory_key=\"history\", return_messages=True),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total tokens: 13772\n",
            "Requests: 1\n",
            "For data file naming conventions, you should adhere to community best practices on data file naming conventions.\n",
            "\n",
            "Citation:\n",
            "- \"A1.1.5 Adhere to community best practice(s) on data file naming conventions\"\n",
            "\n",
            "Procedure:\n",
            "- \"B1.1.5 Define and document file naming conventions using following guidelines: GHRC File Naming convention [DS]\"\n"
          ]
        }
      ],
      "source": [
        "_ = ask(conversation, \"what data file naming conventions should I use?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total tokens: 13823\n",
            "Requests: 1\n",
            "Sure, I will be happy to help. First tell me the core entity or asset in that you will be managing\n",
            "\n",
            "Data \n",
            "Metadata\n",
            "Digital content \n",
            "Code\n",
            "Software\n"
          ]
        }
      ],
      "source": [
        "_ = ask(conversation, \"Can you create a requirements document for me?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total tokens: 13863\n",
            "Requests: 1\n",
            "Tell me about the governance activity need in your project\n",
            "\n",
            "Planning and Design\n",
            "Monitoring\n",
            "Generation/Curation\n",
            "Sharing\n",
            "Use/Reuse\n",
            "Preservation\n"
          ]
        }
      ],
      "source": [
        "_ = ask(conversation, \"Data, Metadata\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total tokens: 13888\n",
            "Requests: 1\n",
            "Are you seeking Requirements or Procedures for your project?\n",
            "\n",
            "Requirements\n",
            "Procedures\n"
          ]
        }
      ],
      "source": [
        "_ = ask(conversation, \"Planning and Design\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total tokens: 14059\n",
            "Requests: 1\n",
            "Here are the headings for the Requirements document:\n",
            "A1.1.1\n",
            "A1.1.2\n",
            "A1.1.3\n",
            "A1.1.4\n",
            "A1.1.5\n",
            "A1.1.6\n",
            "A1.1.7\n",
            "A1.1.8\n",
            "A1.1.9\n",
            "A1.1.10\n",
            "A1.1.11\n",
            "A1.1.12\n",
            "A1.1.13\n",
            "A1.1.14\n",
            "A1.1.15\n",
            "A1.1.16\n",
            "A2.1.1\n",
            "A2.1.2\n",
            "A2.1.3\n",
            "A2.1.4\n",
            "A2.1.5\n",
            "A2.1.6\n"
          ]
        }
      ],
      "source": [
        "model_response = ask(conversation, \"Requirements\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generate MDGF document based on model output\n",
        "- Using regex and string matching to generate the governance document based on the model output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All matches: ['A1.1.1', 'A1.1.2', 'A1.1.3', 'A1.1.4', 'A1.1.5', 'A1.1.6', 'A1.1.7', 'A1.1.8', 'A1.1.9', 'A1.1.10', 'A1.1.11', 'A1.1.12', 'A1.1.13', 'A1.1.14', 'A1.1.15', 'A1.1.16', 'A2.1.1', 'A2.1.2', 'A2.1.3', 'A2.1.4', 'A2.1.5', 'A2.1.6']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "text = model_response\n",
        "\n",
        "pattern = r'[A-Z]\\d+\\.\\d+\\.\\d+[a-z]?'\n",
        "headers = re.findall(pattern, text)\n",
        "print('All matches:', headers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"data\": {\n",
            "        \"plan_design\": [\n",
            "            [\n",
            "                \"A1.1.1 Define a data flow diagram with the purpose of identifying data sources and touchpoints for the project and for communicating to data users how data was handled.\"\n",
            "            ],\n",
            "            [\n",
            "                \"A1.1.2 Develop touchpoint agreements identified in the data flow diagram\"\n",
            "            ],\n",
            "            [\n",
            "                \"A1.1.3 Adhere to community accepted standard machine readable data file formats\"\n",
            "            ],\n",
            "            [\n",
            "                \"A1.1.4 Identify and document all data product characteristics\"\n",
            "            ],\n",
            "            [\n",
            "                \"A1.1.5 Adhere to community best practice(s) on data file naming conventions\"\n",
            "            ],\n",
            "            [\n",
            "                \"A1.1.6 Adhere to community standard variable names, types, and unit(s), keywords\"\n",
            "            ],\n",
            "            [\n",
            "                \"A1.1.7 Adhere to community standards for coordinate systems\"\n",
            "            ],\n",
            "            [\n",
            "                \"A1.1.8 Adhere to community standards for map projections\"\n",
            "            ],\n",
            "            [\n",
            "                \"A1.1.9 Adhere to community standards for date and time formats\"\n",
            "            ],\n",
            "            [\n",
            "                \"A1.1.10 Define a data product versioning scheme\"\n",
            "            ],\n",
            "            [\n",
            "                \"A1.1.11 Define a science quality evaluation plan for data products\"\n",
            "            ],\n",
            "            [\n",
            "                \"A1.1.12 Develop a data retention plan including a process for when and how data will be sunset\"\n",
            "            ],\n",
            "            [\n",
            "                \"A1.1.14 Identify the most appropriate data license for the data product\"\n",
            "            ],\n",
            "            [\n",
            "                \"A1.1.15 Determine content and format for the dataset landing page\"\n",
            "            ],\n",
            "            [\n",
            "                \"A1.1.16 Determine whether API-based data access is needed & if so, identify an API standard\"\n",
            "            ]\n",
            "        ]\n",
            "    },\n",
            "    \"metadata\": {\n",
            "        \"plan_design\": [\n",
            "            [\n",
            "                \"A2.1.1 Adhere to a standard metadata schema for data product (collection) and file (granule) level metadata\"\n",
            "            ],\n",
            "            [\n",
            "                \"A2.1.2 Support mandatory metadata elements in the selected schema\"\n",
            "            ],\n",
            "            [\n",
            "                \"A2.1.3 Utilize a standard data product naming convention\"\n",
            "            ],\n",
            "            [\n",
            "                \"A2.1.4 Identify any needed additional metadata fields for specific projects\"\n",
            "            ],\n",
            "            [\n",
            "                \"A2.1.5 Incorporate any access control fields into the metadata, as required\"\n",
            "            ],\n",
            "            [\n",
            "                \"A2.1.6 Define the data product citation\"\n",
            "            ]\n",
            "        ]\n",
            "    }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "def subset_data(headers, data):\n",
        "    # Initialize a dictionary to hold the subsetted data\n",
        "    subsetted_data = {}\n",
        "    for top_key, top_value in data.items():\n",
        "        if isinstance(top_value, dict):\n",
        "            subsetted_section = {}\n",
        "            \n",
        "            for second_key, entries in top_value.items():\n",
        "                subsetted_entries = []\n",
        "                \n",
        "                for entry_list in entries:\n",
        "                    entry_item = []\n",
        "                    for entry in entry_list:\n",
        "                        if any(header in entry[:10] for header in headers):\n",
        "                            entry_item.append(entry)\n",
        "                    if entry_item:\n",
        "                        subsetted_entries.append(entry_item)\n",
        "                \n",
        "                if subsetted_entries:\n",
        "                    subsetted_section[second_key] = subsetted_entries\n",
        "            if subsetted_section:\n",
        "                subsetted_data[top_key] = subsetted_section\n",
        "    \n",
        "    return subsetted_data\n",
        "\n",
        "import json\n",
        "data = json.load(open(\"../data/dgf.json\"))\n",
        "subset = subset_data(headers, data)\n",
        "\n",
        "# Printing the subset to verify\n",
        "print(json.dumps(subset, indent=4))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "promptlecture",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "f38e0373277d6f71ee44ee8fea5f1d408ad6999fda15d538a69a99a1665a839d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
