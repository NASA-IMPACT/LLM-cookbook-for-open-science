{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ceib8WYZHGF"
      },
      "source": [
        "## Chat with mDGF document \n",
        "\n",
        "This notebook loads the Modern Data Governance Framework (MDGF) document and uses generative models to answer user questions about the document. The notebook also allows creation of governance document based on user input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCeyGy6NZHGG"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# update or install the necessary libraries\n",
        "!pip install --upgrade langchain-openai\n",
        "!pip install --upgrade langchain\n",
        "!pip install --upgrade python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ARDA5TyZHGG",
        "outputId": "b496f8d0-fa8e-4338-f025-835fbd40f2d3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "print(f\".env file loaded correctly: {load_dotenv()}\")\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
        "from langchain.callbacks import get_openai_callback\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
        "from langchain.callbacks import get_openai_callback\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    MessagesPlaceholder\n",
        ")\n",
        "from langchain.schema import (\n",
        "    SystemMessage,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load The MDGF Document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# read the document\n",
        "with open(\"../data/mdgf_document.txt\", \"r\") as file:\n",
        "    mdgf_document = file.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Combining Prompt Patterns\n",
        "- the Persona prompt pattern\n",
        "  - Asking the model to assume persona of expert in scientific data dovernance\n",
        "- Recipe prompt \n",
        "  - Providing the model steps to follow to generate the governance document\n",
        "- Output Automator prompt \n",
        "  - Asking the model to generate a script to automate and provide output in specific format: in this case - the headings required to generate the governance document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hH2VqbsCZHGH",
        "outputId": "aa72381e-7540-481f-d64e-2ce2eb21c088"
      },
      "outputs": [],
      "source": [
        "import pprint\n",
        "import json\n",
        "\n",
        "MDGF_PROMPT = f\"\"\"\n",
        "You are an expert in scientific data governance and management and you will assist the users by answering questions and creating documents. Use only the content in the Modern Data Governance Framework (MDGF) reference text after the delimiter for your answers. If a questions falls outside the reference text, then respond, “This is out of scope for me to answer”\n",
        "\n",
        "Your responsibilities are two::\n",
        "\n",
        "First - Answering Questions:\n",
        "You will be asked questions. Answer the question only using the reference text provided.\n",
        "Apart from Answering the question, Cite the passages from the document used to answer the question, prefixing it with citation.\n",
        "For Any Requirement, you should also provide the corresponding procedure.\n",
        "If you cannot find an answer in the reference text, then respond, “I could not find the answer”\n",
        "\n",
        "Second - Creating Documents:\n",
        "\n",
        "When asked by a user to create either a requirements document or a procedure plan based on the reference text. Assist the user by asking a series of questions to capture their project needs.\n",
        "\n",
        "Step 1: Identify the entity in the user’s project. Respond with: “Sure, I will be happy to help. First tell me the core entity or asset in that you will be managing\n",
        "\n",
        "Data \n",
        "Metadata\n",
        "Digital content \n",
        "Code\n",
        "Software”\n",
        "\n",
        "Step 2: Identify governance activity in the user’s project. Respond with: “Tell me about the governance activity need in your project\n",
        "\n",
        "Planning and Design\n",
        "Monitoring\n",
        "Generation/Curation\n",
        "Sharing\n",
        "Use/Reuse\n",
        "Preservation”\n",
        "\n",
        "Step 3: Identify the user's need for the Type of document. Respond with: “Are you seeking Requirements or Procedures for your project?\n",
        "\n",
        "Requirements\n",
        "Procedures”\n",
        "\n",
        "Finally, Respond with:\n",
        "\"Here are the headings for the Requirements document:\n",
        "A.1.1.1, A.1.2.1, ...\" \n",
        "You should provide only the headings (A.1.1.1, A.1.2.1, ...) provided in the DGF documents. You should never provide any additional information. Do NOT use placeholder text or ... or anything similar in the response.\n",
        "\n",
        "\n",
        "Here is the reference DGF document:\n",
        "{mdgf_document} \n",
        "\"\"\"\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    temperature=0.5,\n",
        "\topenai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
        "\tmodel_name=\"gpt-4-turbo-preview\"\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        SystemMessage(\n",
        "            content=MDGF_PROMPT,\n",
        "        ),  # The persistent system prompt\n",
        "        MessagesPlaceholder(\n",
        "            variable_name=\"history\"\n",
        "        ),  # Where the memory will be stored.\n",
        "        HumanMessagePromptTemplate.from_template(\n",
        "            \"{input}\"\n",
        "        ),  # Where the human input will injected\n",
        "    ]\n",
        ")\n",
        "\n",
        "def ask(chain, query, track_token=True):\n",
        "    with get_openai_callback() as cb:\n",
        "        result = chain.invoke(input=query)\n",
        "        if track_token:\n",
        "            print(f'Total tokens: {cb.total_tokens}')\n",
        "            print(f'Requests: {cb.successful_requests}')\n",
        "    print(result['response'])\n",
        "    return result['response']\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    prompt=prompt,\n",
        "    llm=llm,\n",
        "    verbose=False,\n",
        "    memory=ConversationBufferMemory(ai_prefix=\"AI Assistant\", memory_key=\"history\", return_messages=True),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "_ = ask(conversation, \"what data file naming conventions should I use?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "_ = ask(conversation, \"Can you create a requirements document for me?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "_ = ask(conversation, \"Data, Metadata\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "_ = ask(conversation, \"Planning and Design\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_response = ask(conversation, \"Requirements\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generate MDGF document based on model output\n",
        "- Using regex and string matching to generate the governance document based on the model output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "text = model_response\n",
        "\n",
        "pattern = r'[A-Z]\\d+\\.\\d+\\.\\d+[a-z]?'\n",
        "headers = re.findall(pattern, text)\n",
        "print('All matches:', headers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def subset_data(headers, data):\n",
        "    # Initialize a dictionary to hold the subsetted data\n",
        "    subsetted_data = {}\n",
        "    for top_key, top_value in data.items():\n",
        "        if isinstance(top_value, dict):\n",
        "            subsetted_section = {}\n",
        "            \n",
        "            for second_key, entries in top_value.items():\n",
        "                subsetted_entries = []\n",
        "                \n",
        "                for entry_list in entries:\n",
        "                    entry_item = []\n",
        "                    for entry in entry_list:\n",
        "                        if any(header in entry[:10] for header in headers):\n",
        "                            entry_item.append(entry)\n",
        "                    if entry_item:\n",
        "                        subsetted_entries.append(entry_item)\n",
        "                \n",
        "                if subsetted_entries:\n",
        "                    subsetted_section[second_key] = subsetted_entries\n",
        "            if subsetted_section:\n",
        "                subsetted_data[top_key] = subsetted_section\n",
        "    \n",
        "    return subsetted_data\n",
        "\n",
        "import json\n",
        "data = json.load(open(\"../data/dgf.json\"))\n",
        "subset = subset_data(headers, data)\n",
        "\n",
        "# Printing the subset to verify\n",
        "print(json.dumps(subset, indent=4))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "promptlecture",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "f38e0373277d6f71ee44ee8fea5f1d408ad6999fda15d538a69a99a1665a839d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
