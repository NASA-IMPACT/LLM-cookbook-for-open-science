{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ceib8WYZHGF"
      },
      "source": [
        "## Chat with mDGF document "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lCeyGy6NZHGG"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# update or install the necessary libraries\n",
        "!pip install --upgrade langchain-openai\n",
        "!pip install --upgrade langchain\n",
        "!pip install --upgrade python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ARDA5TyZHGG",
        "outputId": "b496f8d0-fa8e-4338-f025-835fbd40f2d3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import openai\n",
        "import os\n",
        "import IPython\n",
        "from langchain.llms import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tq6ckVHdZHGG"
      },
      "source": [
        "Load environment variables. You can use anything you like but I used `python-dotenv`. Just create a `.env` file with your `OPENAI_API_KEY` then load it as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-yvaExbrZHGG"
      },
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENrupjgNZHGG"
      },
      "source": [
        "We are adapting code from [here](https://langchain.readthedocs.io/en/latest/modules/chat/getting_started.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UwlI72GeZHGG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.chains.conversation.memory import ConversationBufferMemory, ConversationSummaryMemory\n",
        "from langchain.callbacks import get_openai_callback\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    MessagesPlaceholder\n",
        ")\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# read the document\n",
        "with open(\"../data/mdgf_document.txt\", \"r\") as file:\n",
        "    mdgf_document = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hH2VqbsCZHGH",
        "outputId": "aa72381e-7540-481f-d64e-2ce2eb21c088"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts.prompt import PromptTemplate\n",
        "import pprint\n",
        "import json\n",
        "\n",
        "MDGF_PROMPT = f\"\"\"\n",
        "You are an expert in scientific data governance and management and you will assist the users by answering questions and creating documents. Use only the content in the Modern Data Governance Framework (MDGF) reference text after the delimiter for your answers. If a questions falls outside the reference text, then respond, “This is out of scope for me to answer”\n",
        "\n",
        "Your responsibilities are two::\n",
        "\n",
        "First - Answering Questions:\n",
        "You will be asked questions. Answer the question only using the reference text provided.\n",
        "Cite the passage from the document used to answer the question, prefixing it with citation.\n",
        "If you cannot find an answer in the reference text, then respond, “I could not find the answer”\n",
        "\n",
        "Second - Creating Documents:\n",
        "\n",
        "When asked by a user to create either a requirements document or a procedure plan based on the reference text. Assist the user by asking a series of questions to capture their project needs.\n",
        "\n",
        "Step 1: Identify the entity in the user’s project. Respond with: “Sure, I will be happy to help. First tell me the core entity or asset in that you will be managing\n",
        "\n",
        "Data \n",
        "Metadata\n",
        "Digital content \n",
        "Code\n",
        "Software”\n",
        "\n",
        "Step 2: Identify governance activity in the user’s project. Respond with: “Tell me about the governance activity need in your project\n",
        "\n",
        "Planning and Design\n",
        "Monitoring\n",
        "Generation/Curation\n",
        "Sharing\n",
        "Use/Reuse\n",
        "Preservation”\n",
        "\n",
        "Step 3: Identify the user's need for the Type of document. Respond with: “Are you seeking Requirements or Procedures for your project?\n",
        "\n",
        "Requirements\n",
        "Procedures”\n",
        "\n",
        "Finally, Respond with:\n",
        "\"Here are the headings for the Requirements document:\n",
        "A.1.1.1, A.1.2.1, ...\" \n",
        "You should provide only the headings (A.1.1.1, A.1.2.1, ...) provided in the DGF documents. You should never provide any additional information. Do NOT use placeholder text or ... or anything similar in the response.\n",
        "\n",
        "\n",
        "Here is the reference DGF document:\n",
        "{mdgf_document} \n",
        "\"\"\"\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    temperature=0,\n",
        "\topenai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
        "\tmodel_name=\"gpt-4-turbo-preview\"\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        SystemMessage(\n",
        "            content=MDGF_PROMPT,\n",
        "        ),  # The persistent system prompt\n",
        "        MessagesPlaceholder(\n",
        "            variable_name=\"history\"\n",
        "        ),  # Where the memory will be stored.\n",
        "        HumanMessagePromptTemplate.from_template(\n",
        "            \"{input}\"\n",
        "        ),  # Where the human input will injected\n",
        "    ]\n",
        ")\n",
        "\n",
        "def ask(chain, query, track_token=True):\n",
        "    with get_openai_callback() as cb:\n",
        "        result = chain.invoke(input=query)\n",
        "        if track_token:\n",
        "            print(f'Total tokens: {cb.total_tokens}')\n",
        "            print(f'Requests: {cb.successful_requests}')\n",
        "    print(result['response'])\n",
        "    return result['response']\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    prompt=prompt,\n",
        "    llm=llm,\n",
        "    verbose=False,\n",
        "    memory=ConversationBufferMemory(ai_prefix=\"AI Assistant\", memory_key=\"history\", return_messages=True),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total tokens: 13719\n",
            "Requests: 1\n",
            "Sure, I will be happy to help. First tell me the core entity or asset in that you will be managing\n",
            "\n",
            "- Data \n",
            "- Metadata\n",
            "- Digital content \n",
            "- Code\n",
            "- Software\n"
          ]
        }
      ],
      "source": [
        "_ = ask(conversation, \"help me create a MDGF format document\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total tokens: 13763\n",
            "Requests: 1\n",
            "Tell me about the governance activity need in your project\n",
            "\n",
            "- Planning and Design\n",
            "- Monitoring\n",
            "- Generation/Curation\n",
            "- Sharing\n",
            "- Use/Reuse\n",
            "- Preservation\n"
          ]
        }
      ],
      "source": [
        "_ = ask(conversation, \"Digital content\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total tokens: 13789\n",
            "Requests: 1\n",
            "Are you seeking Requirements or Procedures for your project?\n",
            "\n",
            "- Requirements\n",
            "- Procedures\n"
          ]
        }
      ],
      "source": [
        "_ = ask(conversation, \"Planning and Design\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total tokens: 13866\n",
            "Requests: 1\n",
            "Here are the headings for the Requirements document:\n",
            "A3.1.1, A3.1.2, A3.1.3\n",
            "\n",
            "Here are the headings for the Procedures document:\n",
            "B3.1.1, B3.1.2a, B3.1.2b, B3.1.3\n"
          ]
        }
      ],
      "source": [
        "model_response = ask(conversation, \"both\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Match found: A1.1.1\n",
            "Match found: A1.1.2\n",
            "Match found: A1.1.3\n",
            "Match found: A1.1.4\n",
            "Match found: A1.1.5\n",
            "Match found: A1.1.6\n",
            "Match found: A1.1.7\n",
            "Match found: A1.1.8\n",
            "Match found: A1.1.9\n",
            "Match found: A1.1.10\n",
            "Match found: A1.1.11\n",
            "Match found: A1.1.12\n",
            "Match found: A1.1.13\n",
            "Match found: A1.1.14\n",
            "Match found: A1.1.15\n",
            "Match found: A1.1.16\n",
            "Match found: B1.1.1\n",
            "Match found: B1.1.2\n",
            "Match found: B1.1.3\n",
            "Match found: B1.1.4\n",
            "Match found: B1.1.5\n",
            "Match found: B1.1.6\n",
            "Match found: B1.1.7\n",
            "Match found: B1.1.8\n",
            "Match found: B1.1.9\n",
            "Match found: B1.1.10\n",
            "Match found: B1.1.11\n",
            "Match found: B1.1.12\n",
            "Match found: B1.1.13\n",
            "Match found: B1.1.14\n",
            "Match found: B1.1.15\n",
            "Match found: B1.1.16\n",
            "All matches: ['A1.1.1', 'A1.1.2', 'A1.1.3', 'A1.1.4', 'A1.1.5', 'A1.1.6', 'A1.1.7', 'A1.1.8', 'A1.1.9', 'A1.1.10', 'A1.1.11', 'A1.1.12', 'A1.1.13', 'A1.1.14', 'A1.1.15', 'A1.1.16', 'B1.1.1', 'B1.1.2', 'B1.1.3', 'B1.1.4', 'B1.1.5', 'B1.1.6', 'B1.1.7', 'B1.1.8', 'B1.1.9', 'B1.1.10', 'B1.1.11', 'B1.1.12', 'B1.1.13', 'B1.1.14', 'B1.1.15', 'B1.1.16']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# Example text to search\n",
        "text = _\n",
        "\n",
        "# Regex pattern to match patterns like \"B 1.2.2b\"\n",
        "pattern = r'[A-Z]\\d+\\.\\d+\\.\\d+[a-z]?'\n",
        "\n",
        "# Using finditer() to find matches and print them\n",
        "for match in re.finditer(pattern, text):\n",
        "    print('Match found:', match.group())\n",
        "\n",
        "# Alternatively, using findall() to get a list of all matches\n",
        "headers = re.findall(pattern, text)\n",
        "print('All matches:', headers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"data\": {\n",
            "        \"plan_design\": [\n",
            "            [\n",
            "                \"A1.1.1 Define a data flow diagram with the purpose of identifying data sources and touchpoints for the project and for communicating to data users how data was handled.\",\n",
            "                \"B1.1.1 Create a data flow diagram extending from acquisition/creation to user delivery and add diagram to DMP. [DE] Example diagram:\"\n",
            "            ],\n",
            "            [\n",
            "                \"A1.1.2 Develop touchpoint agreements identified in the data flow diagram\",\n",
            "                \"B1.1.2 Create needed touchpoint agreements such as Interface Control Documents, (ICDs) / Submission Agreement (SA), Memorandum of Understanding (MOU),or Service Level Agreement (SLA). [DS + DE]\"\n",
            "            ],\n",
            "            [\n",
            "                \"A1.1.3 Adhere to community accepted standard machine readable data file formats\",\n",
            "                \"B1.1.3 Select standard machine-readable data file format(s) from NASA Approved Data Formats [DS] The EOSDIS Data Product Development Guide for Data Producers - Quick Start Guide prefers the following data formats: netCDF-4 and GeoTIFF. Cloud Optimized GeoTIFF (COG) and Zarr are the preferred Cloud-based formats\"\n",
            "            ],\n",
            "            [\n",
            "                \"A1.1.4 Identify and document all data product characteristics\",\n",
            "                \"B1.1.4 Create a data sheet using the following template: Data Sheet Template. Add additional data characteristics as needed for each project. [DS]\"\n",
            "            ],\n",
            "            [\n",
            "                \"A1.1.5 Adhere to community best practice(s) on data file naming conventions\",\n",
            "                \"B1.1.5 Define and document file naming conventions using following guidelines: GHRC File Naming convention [DS]\"\n",
            "            ],\n",
            "            [\n",
            "                \"A1.1.6 Adhere to community standard variable names, types, and unit(s), keywords\",\n",
            "                \"B1.1.6 Utilize standard variable(s), types, and unit(s) such as CF convention [DS]\"\n",
            "            ],\n",
            "            [\n",
            "                \"A1.1.7 Adhere to community standards for coordinate systems\",\n",
            "                \"B1.1.7 Utilize coordinate reference systems (CRS) from this list (https://epsg.io/) [DE] Recommended global CRS: 2-dimensional World Geodetic System 1984 (WGS 84) (Lat/Long): EPSG:4326 WGS 84 World Mercator: EPSG:3395 WGS 84 Pseudo-Mercator: EPSG:3857 3-dimensional WGS 84 (Lat/Long/Elevation): EPSG:4979 Recommended CRS for data over polar regions: WGS 84 Arctic Polar Stereographic: EPSG:3995 NSIDC Sea Ice Polar Stereographic North: EPSG:3413 NSIDC Sea Ice Polar Stereographic South: EPSG:3976\"\n",
            "            ],\n",
            "            [\n",
            "                \"A1.1.8 Adhere to community standards for map projections\",\n",
            "                \"B1.1.8 Utilize map projections from this list (https://epsg.io/) [DE]\"\n",
            "            ],\n",
            "            [\n",
            "                \"A1.1.9 Adhere to community standards for date and time formats\",\n",
            "                \"B1.1.9 Utilize data and time formatting from ISO 8601 [DE]\"\n",
            "            ],\n",
            "            [\n",
            "                \"A1.1.10 Define a data product versioning scheme\",\n",
            "                \"B1.1.10 Represent the data product version with an ordinal identifier (e.g., 1, 2, 3, etc.) that expresses its position in a series of data product publications. The data product version can be represented with both a major and minor version identifier (e.g., 2.1, 2.2, etc.). (Reference: see Section 4.3 of the Data Product Development Guide for Data Producers) [DE]\"\n",
            "            ],\n",
            "            [\n",
            "                \"A1.1.11 Define a science quality evaluation plan for data products\",\n",
            "                \"B1.1.11 Develop the characteristics of the science quality evaluation needed for each data product [DS] Suggested: Univariate visualization of each field in the raw dataset, with summary statistics and Fill Values, Mask Values\"\n",
            "            ],\n",
            "            [\n",
            "                \"A1.1.12 Develop a data retention plan including a process for when and how data will be sunset\",\n",
            "                \"B1.1.12 Create a data retention plan that includes information about the end of project preservation plan and rolling archive plans. Use the CSDA data retirement policy template as needed. [DS]\"\n",
            "            ],\n",
            "            [\n",
            "                \"B1.1.13 Develop a metrics implementation plan. [DE] Recommended minimum metrics: Data Use Metrics Data Product Search frequency S3 Bucket Access frequency Data download counts Information/Data Profile: Data completeness Metadata completeness Data lineage completeness Data Quality Checksum validation Data Processing Processing time Processing throughput Error rate Resource utilization Ingest: Ingest rate Ingest completeness / volume Ingest error rate Data Access APIs/Services: Service availability Service usage Service response time Service error rate\"\n",
            "            ],\n",
            "            [\n",
            "                \"A1.1.14 Identify the most appropriate data license for the data product\",\n",
            "                \"B1.1.14 If there are no other restrictions, SMD scientific data should be released with a Creative Commons Zero license. [DS]\"\n",
            "            ],\n",
            "            [\n",
            "                \"A1.1.15 Determine content and format for the dataset landing page\",\n",
            "                \"B1.1.15 Design dataset landing page format and content. Recommend using the IMPACT data product landing page design. [DS] Note that dataset landing pages can be automatically generated using UMM metadata (published to CMR) and STAC metadata (using STAC Browser). All information needed in the dataset landing page should be included in the metadata.\"\n",
            "            ],\n",
            "            [\n",
            "                \"A1.1.16 Determine whether API-based data access is needed & if so, identify an API standard\",\n",
            "                \"B1.1.16a Refer to your system design as to whether an API-based data access is needed. [DE] For example, databases that store vector data should have an API. B1.1.16b If an API doesn\\u2019t already exist for the data being distributed, select an appropriate OGC API standard to use (also see Requirement A1.4.2). For raster and map content, use OGC API - Maps. For vector and tile data, use OGC API - Tiles. Also consider using OGC API - Features as needed.[DE]\"\n",
            "            ]\n",
            "        ]\n",
            "    }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Your list of strings (headers you're interested in)\n",
        "# headers = ['A4.3.1', 'B4.3.2a', 'A1.1.3']\n",
        "\n",
        "# Your JSON data\n",
        "data = {\n",
        "    \"code\": {\n",
        "        \"sharing\": [\n",
        "            [\"A4.3.1 Ensure that the code is openly accessible\", \"B4.3.1 Ensure the code repository is set to ‘public’ in GitHub. [DE]\"],\n",
        "            [\"A4.3.2 Ensure that the code has a persistent identifier and is discoverable with the data\", \"B4.3.2a The code repository should be assigned a registered persistent identifier. Use Zenodo for assigning a new DOI. In rare cases that a DOI has been assigned via other mechanisms, ensure the DOI is uploaded to the repo. [DS] B4.3.2b Ensure the code identifier is added to the data product metadata. [DS] B4.3.2c Ensure the DOI is added to the Github citation file [DS + DE]\"],\n",
        "            [\"A4.3.3 Ensure the code is documented\", \"B4.3.3a Include a read me document that describes the purpose of the code and any system requirements [DE] B4.3.3b Include a brief ‘About’ description of the code that will be displayed towards the top of the repo page. The description should be no more than 325 characters or 50 words. [DS] Example ‘About’ text.\"]\n",
        "        ],\n",
        "        \"generation_curation\": [\n",
        "            [\"A4.2.1 Develop code in accordance with current best practices\", \"B4.2.1 Use the IMPACT coding best practices [DE]\"],\n",
        "            [\"A4.2.2 Ensure code is citable\", \"B4.2.2a Create a clear, sufficiently descriptive name for your code repo [DE] B4.2.2b Create a citation file for all code with information identified in B4.1.6. [DS]\"]\n",
        "        ]\n",
        "        # ... other sections\n",
        "    }\n",
        "    # ... other top-level keys\n",
        "}\n",
        "\n",
        "def subset_data(headers, data):\n",
        "    # Initialize a dictionary to hold the subsetted data\n",
        "    subsetted_data = {}\n",
        "    for top_key, top_value in data.items():\n",
        "        if isinstance(top_value, dict):\n",
        "            subsetted_section = {}\n",
        "            \n",
        "            for second_key, entries in top_value.items():\n",
        "                subsetted_entries = []\n",
        "                \n",
        "                for entry_list in entries:\n",
        "                    entry_item = []\n",
        "                    for entry in entry_list:\n",
        "                        if any(header in entry[:10] for header in headers):\n",
        "                            entry_item.append(entry)\n",
        "                    if entry_item:\n",
        "                        subsetted_entries.append(entry_item)\n",
        "                \n",
        "                if subsetted_entries:\n",
        "                    subsetted_section[second_key] = subsetted_entries\n",
        "            if subsetted_section:\n",
        "                subsetted_data[top_key] = subsetted_section\n",
        "    \n",
        "    return subsetted_data\n",
        "\n",
        "import json\n",
        "\n",
        "data = json.load(open(\"../data/dgf.json\"))\n",
        "\n",
        "subset = subset_data(headers, data)\n",
        "\n",
        "# Printing the subset to verify\n",
        "print(json.dumps(subset, indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MDGF_QA_PROMPT = f\"\"\"\n",
        "You are an expert in scientific data governance and management and you will assist the users by answering questions. Use only the content in the Modern Data Governance Framework (MDGF) reference text provided for your answers. If a questions falls outside the reference text, then respond, “This is out of scope for me to answer”\n",
        "\n",
        "Here is the reference DGF document:\n",
        "{mdgf_document} \n",
        "\"\"\"\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    temperature=0,\n",
        "\topenai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
        "\tmodel_name=\"gpt-4-turbo-preview\"\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        SystemMessage(\n",
        "            content=MDGF_QA_PROMPT,\n",
        "        ),  # The persistent system prompt\n",
        "        MessagesPlaceholder(\n",
        "            variable_name=\"history\"\n",
        "        ),  # Where the memory will be stored.\n",
        "        HumanMessagePromptTemplate.from_template(\n",
        "            \"{input}\"\n",
        "        ),  # Where the human input will injected\n",
        "    ]\n",
        ")\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    prompt=prompt,\n",
        "    llm=llm,\n",
        "    verbose=False,\n",
        "    memory=ConversationBufferMemory(ai_prefix=\"AI Assistant\", memory_key=\"history\", return_messages=True),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MDGF_DOC_PROMPT = f\"\"\"\n",
        "You are responsible for providing a JSON with the following schema:\n",
        ",\n",
        "{\n",
        "    \"Entities/Assets\": List[{\n",
        "        \n",
        "    }],\n",
        "    \n",
        "    \"Type\": List[str]\n",
        "}\n",
        "\n",
        "When asked by a user to create either a requirements document or a procedure plan based on the reference text. Assist the user by asking a series of questions to capture their project needs.\n",
        "\n",
        "Step 1: Identify the entities in the user’s project. Respond with: “Sure, I will be happy to help. First tell me the core entities or assets in that you will be managing\n",
        "\n",
        "Data \n",
        "Metadata\n",
        "Digital content \n",
        "Code\n",
        "Software”\n",
        "\n",
        "Step 2: Identify governance activities in the user’s project. Respond with: “Tell me about the governance activities need in your project\n",
        "\n",
        "Planning and Design\n",
        "Monitoring\n",
        "Generation/Curation\n",
        "Sharing\n",
        "Use/Reuse\n",
        "Preservation”\n",
        "\n",
        "Step 3: Identify the user's need for the Type of document. Respond with: “Are you seeking Requirements or Procedures for your project?\n",
        "\n",
        "Requirements\n",
        "Procedures”\n",
        "\n",
        "Finally. Respond with: “I will provide only the headings provided in the DGF documents to respond to your request. I will not provide the content under each heading.\n",
        " I will provide A JSON formatted document with the keys: Entities/Assets, Governance Activities, Type. and values as the user's response to the questions asked in the previous steps.\n",
        "here is the JSON formatted document:\n",
        "{\n",
        "  \"Entities/Assets\": \"\",\n",
        "  \"Governance Activities\": \" \",\n",
        "  \"Type\": \" \"\n",
        "}\n",
        "...\n",
        "\" Correctly Fill in all the needed headings here. Do not use filler text or ... in the response.\n",
        "\n",
        "\n",
        "\n",
        "Here is the reference DGF document:\n",
        "{mdgf_document} \n",
        "\"\"\"\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    temperature=0,\n",
        "\topenai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
        "\tmodel_name=\"gpt-4-turbo-preview\"\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        SystemMessage(\n",
        "            content=MDGF_QA_PROMPT,\n",
        "        ),  # The persistent system prompt\n",
        "        MessagesPlaceholder(\n",
        "            variable_name=\"history\"\n",
        "        ),  # Where the memory will be stored.\n",
        "        HumanMessagePromptTemplate.from_template(\n",
        "            \"{input}\"\n",
        "        ),  # Where the human input will injected\n",
        "    ]\n",
        ")\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    prompt=prompt,\n",
        "    llm=llm,\n",
        "    verbose=False,\n",
        "    memory=ConversationBufferMemory(ai_prefix=\"AI Assistant\", memory_key=\"history\", return_messages=True),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pydantic import BaseModel\n",
        "from typing import List, Dict, Optional\n",
        "\n",
        "class DocType(BaseModel):\n",
        "    requirements: str\n",
        "    procedures: str\n",
        "\n",
        "class GovernanceActivities(BaseModel):\n",
        "    planning_and_design: Optional[List[DocType]] = None\n",
        "    monitoring: Optional[List[DocType]] = None\n",
        "    generation_curation: Optional[List[DocType]] = None\n",
        "    sharing: Optional[List[DocType]] = None\n",
        "    use_reuse: Optional[List[DocType]] = None\n",
        "    preservation: Optional[List[DocType]] = None\n",
        "\n",
        "class CoreEntity(BaseModel):\n",
        "    data: Optional[List[GovernanceActivities]] = None\n",
        "    metadata: Optional[List[GovernanceActivities]] = None\n",
        "    digital_content: Optional[List[GovernanceActivities]] = None\n",
        "    code: Optional[List[GovernanceActivities]] = None\n",
        "    software: Optional[List[GovernanceActivities]] = None\n",
        "\n",
        "from typing import Dict, Any\n",
        "\n",
        "def json_to_pydantic(json_dict: Dict[str, Any]) -> CoreEntity:\n",
        "    def create_doc_type_list(details: List[List[str]]) -> List[DocType]:\n",
        "        # This function takes a list of lists where each inner list contains requirements and procedures.\n",
        "        return [DocType(requirements=detail[0], procedures=detail[1]) for detail in details]\n",
        "\n",
        "    def create_governance_activities(activities: Dict[str, List[List[str]]]) -> GovernanceActivities:\n",
        "        # This function creates a GovernanceActivities instance from a dictionary mapping activity names to lists of [requirements, procedures].\n",
        "        activities_dict = {activity: create_doc_type_list(details) for activity, details in activities.items()}\n",
        "        return GovernanceActivities(**activities_dict)\n",
        "\n",
        "    # The corrected part: Ensure we correctly parse the structure of each core entity.\n",
        "    core_entities = {}\n",
        "    for entity, activities_list in json_dict.items():\n",
        "        if isinstance(activities_list, list):  # Ensure it's the expected list structure\n",
        "            core_entities[entity] = [create_governance_activities(activities) for activities in activities_list]\n",
        "\n",
        "    return CoreEntity(**core_entities)\n",
        "\n",
        "json_data = {\n",
        "    \"code\": [{\n",
        "        \"sharing\": [\n",
        "            [\n",
        "                \"A4.3.1 Ensure that the code is openly accessible\",\n",
        "                \"B4.3.1 Ensure the code repository is set to 'public' in GitHub. [DE]\"\n",
        "            ],\n",
        "            [\n",
        "                \"A4.3.2 Ensure that the code has a persistent identifier and is discoverable with the data\",\n",
        "                \"B4.3.2a The code repository should be assigned a registered persistent identifier. Use Zenodo for assigning a new DOI. In rare cases that a DOI has been assigned via other mechanisms, ensure the DOI is uploaded to the repo. [DS] B4.3.2b Ensure the code identifier is added to the data product metadata. [DS] B4.3.2c Ensure the DOI is added to the Github citation file [DS + DE]\"\n",
        "            ],\n",
        "            [\n",
        "                \"A4.3.3 Ensure the code is documented\",\n",
        "                \"B4.3.3a Include a read me document that describes the purpose of the code and any system requirements [DE] B4.3.3b Include a brief ‘About’ description of the code that will be displayed towards the top of the repo page. The description should be no more than 325 characters or 50 words. [DS]\"\n",
        "            ]\n",
        "        ]\n",
        "    }]\n",
        "}\n",
        "\n",
        "pydantic_model_instance = json_to_pydantic(json_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "dgf_json = json.load(open(\"../data/dgf.json\"))\n",
        "\n",
        "pydantic_model_instance = json_to_pydantic(dgf_json)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CoreEntity(data=None, metadata=None, digital_content=None, code=None, software=None)"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pydantic_model_instance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjHsxmRsZHGH"
      },
      "source": [
        "Let's try an example that involves a system instruction and a task provided by user."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "promptlecture",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "f38e0373277d6f71ee44ee8fea5f1d408ad6999fda15d538a69a99a1665a839d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
