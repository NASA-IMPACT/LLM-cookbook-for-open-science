Modern Data Governance Framework (mDGF)

Background

A Data Governance Framework serves as a template or starting point for organizations to define and enforce policies that govern the management of data throughout its lifecycle. Requirements and implementation procedures captured in this document should be included in an Open Science Management Plan for Data (i.e., OSDMP) and/or a Software Management Plan (i.e., SMP) or a Data Management Plan (DMP) when initiating a new IMPACT project.

Governance Components

Understanding the core entities (or assets) you are managing
Data - any scientific measurement. This includes Earth observation data, scientific images and model output data
Metadata - structured information about the data, 
Digital content - Information provided in digital documents that pertain to data or the scientific process. This includes documentation, algorithms, technical reports, and any ancillary information or work product related to the data.
Code - a set of instructions used to execute computation i.e. (python programs, notebooks, libraries, run scripts)
Software - to be revisited later

Note: Architecture is deemed to be out of scope for this framework and should be included in the project management plan.

Understanding the Resources you have?
Resources are your constraints. These constraints are important to consider as they impact governance of the assets and as such have to be managed.
Storage - where data, code, metadata and information are stored (i.e Object Store (S3) or On Premise)
People - the mDGF breaks down tasks and skill sets into two broad roles. The roles are:
Data stewards - The data steward focuses on ensuring compliance to governance policies and domain specific standards. In effect, the steward is responsible for implementing governance policies and monitoring information assets against those policies. The data steward is responsible for curating metadata, gathering and organizing information, adhering to open science principles and executing against the Data Governance Plan. 
Data engineers - Data engineers are primarily responsible for building, managing and operationalizing data pipelines The data engineer is responsible for intaking, checking, reprocessing data, implementing services to make data, metadata, information accessible.
Compute - to be added later

Governance Activities 

Understanding the activities that you or your team will have to perform

These activities can be of two types:

Foundational : These activities are needed by all projects and inform the data lifecycle
Planning and Design - plan & design what needs to be done for your project at each lifecycle phase
Determine appropriate lifecycle phases
Align with requirements and policies
Must consider external constraints such as architecture and other resources (people, costs, storage)
Monitoring - periodic assessment of the entities being managed as well as the entire governance process
Metrics
Quality
Security (access controls)
Resources
Data LifeCycle : These activities are implemented at each phase of the data lifecycle
Generation/Curation - activities associated with collecting, creating, producing, acquiring, and curating core entities. These activities include: Collection, Creation, Production, Acquisition, Curation, Ingestion, Storage
Data collection is the systematic process of gathering and measuring information on targeted variables in an established system, which enables one to answer relevant questions and evaluate outcomes. 
Data creation refers to the process of producing new data or information that didn't exist before. In scientific research, data creation could refer to the process of conducting experiments or observations that generate new data. 
Data production refers to the processes and methods involved in generating and preparing data for use in analysis.
Data acquisition refers to the process of gathering, filtering, and cleaning data from various sources to be used for data analysis or decision-making purposes. The process involves collecting raw data from within the system or from external sources, transforming it into usable data, and loading it into a structured form for further analysis or processing.
Data curation refers to the process of organizing, integrating, cleaning, and enhancing raw data to create high-quality datasets that are easy to use and can be trusted to support decision-making, research, and analysis.
Data ingestion refers to the process of importing, transferring, loading, and processing data for later use or storage in a structured form. This process involves taking raw data from various sources and bringing it into a system where it can be analyzed and used to inform decision-making.
Data storage refers to the use of recording media to retain digital data within a data storage device.
Sharing - activities associated with making the core entities discoverable and accessible. Activities include:
Publication of entities to an open, searchable catalog or a repository
Dissemination/communication of information to stakeholders
Ensuring access to the core entities
Use/Reuse - activities associated with making core entities usable to stakeholders. Activities for interacting with the core entities include: 
Exploration - data, metadata, information, code
Execution - code
Analysis - data, metadata
Visualization - data
Preservation - activities associated with long term care of the core entities. Activities include: 
Archiving
Maintenance
Disposal


Te mDGF structure and the numbering system is structured that the first number stands for core entities, the next number stands for activities. A - stands for requirements and B - stands for the procedures that need to be implemented to meet the requirement.  For example A 1.2.1 would mean requirement for entity ‘data’ and activity curation




1. Data

1.1 Planning and Design

Requirements

A1.1.1  Define a data flow diagram with the purpose of identifying data sources and touchpoints for the project and for communicating to data users how data was handled.
A1.1.2 Develop touchpoint agreements identified in the data flow diagram
A1.1.3 Adhere to community accepted standard machine readable data file formats
A1.1.4 Identify and document all data product characteristics
A1.1.5 Adhere to community best practice(s) on data file naming conventions
A1.1.6 Adhere to community standard variable names, types, and unit(s), keywords
A1.1.7 Adhere to community standards for coordinate systems
A1.1.8 Adhere to community standards for map projections
A1.1.9 Adhere to community standards for date and time formats
A1.1.10 Define a data product versioning scheme
A1.1.11 Define a science quality evaluation plan for data products
A1.1.12 Develop a data retention plan including a process for when and how data will be sunset
A1.1.13 Define metrics to be collected along the following dimensions:
Data use (search and access)
Data quality
Data/information (quality) profile
Data Processing
Ingest 
Data Access APIs/Services
A1.1.14 Identify the most appropriate data license for the data product
A1.1.15 Determine content and format for the dataset landing page
A1.1.16 Determine whether API-based data access is needed & if so, identify an API standard

Procedures

B1.1.1 Create a data flow diagram extending from acquisition/creation to user delivery and add diagram to DMP. [DE]
B1.1.2 Create needed touchpoint agreements such as Interface Control Documents,  (ICDs) / Submission Agreement (SA), Memorandum of Understanding (MOU),or Service Level Agreement (SLA).  [DS + DE]
B1.1.3 Select standard machine-readable data file format(s) from NASA Approved Data Formats [DS]
The EOSDIS Data Product Development Guide for Data Producers - Quick Start Guide prefers the following data formats: netCDF-4 and GeoTIFF. 
Cloud Optimized GeoTIFF (COG) and Zarr are the preferred Cloud-based formats
B1.1.4 Create a data sheet using the following template: Data Sheet Template. Add additional data characteristics as needed for each project. [DS]
B1.1.5  Define and document file naming conventions using following guidelines: GHRC File Naming convention [DS]
B1.1.6 Utilize standard variable(s), types, and unit(s) such as CF convention [DS]
B1.1.7 Utilize coordinate reference systems (CRS) from this list (https://epsg.io/) [DE]
Recommended global CRS:
2-dimensional World Geodetic System 1984 (WGS 84) (Lat/Long):  EPSG:4326
WGS 84 World Mercator: EPSG:3395
WGS 84 Pseudo-Mercator: EPSG:3857
3-dimensional WGS 84 (Lat/Long/Elevation): EPSG:4979
Recommended CRS for data over polar regions:
WGS 84 Arctic Polar Stereographic: EPSG:3995
NSIDC Sea Ice Polar Stereographic North: EPSG:3413
NSIDC Sea Ice Polar Stereographic South: EPSG:3976
B1.1.8 Utilize map projections from this list (https://epsg.io/) [DE]
B1.1.9 Utilize data and time formatting from ISO 8601 [DE]
B1.1.10 Represent the data product version with an ordinal identifier (e.g., 1, 2, 3, etc.) that expresses its position in a series of data product publications. The data product version can be represented with both a major and minor version identifier (e.g., 2.1, 2.2, etc.). (Reference: see Section 4.3 of the Data Product Development Guide for Data Producers) [DE]
B1.1.11 Develop the characteristics of the science quality evaluation needed for each data product [DS]
Suggested: 
Univariate visualization of each field in the raw dataset, with summary statistics and Fill Values, Mask Values
B1.1.12 Create a data retention plan that includes information about the end of project preservation plan and rolling archive plans. Use the CSDA data retirement policy template as needed. [DS]
B1.1.13 Develop a metrics implementation plan. [DE] 

Recommended minimum metrics:
Data Use Metrics
Data Product Search frequency
S3 Bucket Access frequency
Data download counts
Information/Data Profile:
Data completeness
Metadata completeness
Data lineage completeness
Data Quality
Checksum validation
Data Processing
Processing time
Processing throughput
Error rate
Resource utilization
Ingest:
Ingest rate
Ingest completeness / volume
Ingest error rate
Data Access APIs/Services:
Service availability
Service usage
Service response time
Service error rate
B1.1.14 If there are no other restrictions, SMD scientific data should be released with a Creative Commons Zero license. [DS]
B1.1.15 Design dataset landing page format and content. Recommend using the IMPACT data product landing page design. [DS] 
Note that dataset landing pages can be automatically generated using UMM metadata (published to CMR) and STAC metadata (using STAC Browser). All information needed in the dataset landing page should be included in the metadata.
B1.1.16a Refer to your system design as to whether an API-based data access is needed. [DE] For example, databases that store vector data should have an API.
B1.1.16b If an API doesn’t already exist for the data being distributed, select an appropriate OGC API standard to use (also see Requirement A1.4.2). For raster and map content, use OGC API - Maps. For vector and tile data, use OGC API - Tiles. Also consider using OGC API - Features as needed.[DE]




1.2 Generation / Curation (Collection, Creation, Production, Acquisition, Curation, Ingestion, Storage)

Requirements

A1.2.1 Ensure complete and accurate ingest of data
A1.2.2 Ensure complete and accurate production of data
A1.2.3 	Validate that Planning and Design requirements (from Section A1.1) are met for data ingest and / or production 
A1.2.4 Ensure data products are assigned persistent identifiers and cross-referenced if appropriate

Procedures

B1.2.1a Obtain a manifest file from the data provider. The manifest file should contain total file numbers and checksums of individual files using an SHA algorithm (recommend at minimum SHA256) [DE]
B1.2.1b Confirm number of ingested files matches source [DE]
B1.2.1c Perform checksums and confirm match to source [DE]
B1.2.1d Generate an ingest report [DE]
An ingest report is a document that serves as a receipt verifying that the ingested data matches the manifest file from the data provider (see B1.2.1a). It should include a list of files with checksums, data volume, and date/time of ingest. This can be shared with the data provider if needed. 
The report should be in JSON format with key/value pairs
B1.2.2a Create checksums / manifest file during data production [DE]
B1.2.2b Perform planned science quality checks for the data produced before release (see Requirement A1.1.11) [DE]
B1.2.3 Perform review demonstrating implementation has met the defined requirements from the planning phase (i.e. everything specified for Requirements A1.1.3 through A1.1.16; data format, file naming conventions, etc.) [DE]
B1.2.4 Include the persistent identifier (e.g. DOI) in the data product metadata [DS]
ESDIS DOI Process: Follow the ESDS process when delivering data to a DAAC. DOIs should be reserved prior to the delivery of actual data to a DAAC. Detailed steps for registering/reserving DOIs with ESDS as well as updating a DOI are described in the ESDS DOI Process.
Zenodo Process: Follow this process for data not going to a DAAC; More details on how to upload data to Zenodo are described in this Zenodo best practice document.



1.3 Data Sharing

Requirements

A1.3.1 Ensure data product identifiers are resolvable
A1.3.2 Ensure policy-based access to data
A1.3.3 Ensure all shared data products are self-describing with sufficient structural metadata for data use
A1.3.4 Ensure all data product files are accessible (user registration may be required)
A1.3.5 Ensure checksum and manifest file is available to users (for users to verify downloaded files)

Procedures

B1.3.1a Check that the persistent identifier for the data resolves to the appropriate dataset landing page (see A3.1.1) The persistent identifier should be found somewhere on the landing page, but is ideally embedded in schema.org markup such as JSON-LD and/or using HTML meta tags. [DE]
Example schema.org/JSON-LD:
	<application type="application/ld+json"> 
{"@id": "https://doi.org/10.5067/GHGMR-4FJ04"} 
</application>
Example HTML meta tags:
<meta name="DC.identifier" content="https://doi.org/10.5067/GHGMR-4FJ04">

Please note: In the examples shown above, it should be the DOI url (i.e. https://doi.org/10.5067/GHGMR-4FJ04), not the DOI value (i.e., 10.5067/GHGMR-4FJ04). JSON-LD (JavaScript Object Notation for Linked Data) is a lightweight data interchange format used to structure and represent data on the web - W3C (World Wide Web Consortium) standard.

B1.3.1b The landing page should use RDFa – lite semantic annotations (https://www.w3.org/TR/rdfa-lite/) [DE]

RDF (Resource Description Framework) is a data model and framework for describing resources on the web and their relationships in a standardized way. RDFa (RDF in Attributes) is an extension of HTML that allows you to embed RDF data within HTML documents using attributes in the markup. Both RDF and RDFa are W3C standards. 

An example of including an open CC-BY-4.0 license:
 …
<p>All content on this site is licensed under
   <a property="https://creativecommons.org/ns#license" 
             href="https://creativecommons.org/licenses/by/4.0/">
   		  a Creative Commons License</a>.</p>
…
B1.3.2 Implement enforcement of access restrictions [as specified in End User License Agreement (EULA) or SPD-41a Open Access] [DE]
B1.3.3 Use the Climate Forecast (CF) to provide structural metadata. Best practices for CF global metadata attributes are also available.  [DS + DE] 
B1.3.4a Check that data access links are working [DE]
B1.3.4b Check that data access APIs are working [DE]
B1.3.4c Implement secure transfer protocol access in compliance with U.S. government policy [DE]
B1.3.5 Place the manifest file with data (see Requirement A1.2.1 and A1.2.2) [DE]



1.4 Data Use/Reuse

Requirements
A1.4.1 Use a clear data versioning scheme to ensure that users are aware of significant changes to data

Procedures
B1.4.1 Provide users with data product change logs using a template similar to this example. [DS  +DE]

1.5 Data Preservation (archive, maintenance, disposal)

Requirements

A1.5.1 Preserve data for long-term use following community standards and NASA data preservation policy

Procedures

B1.5.1a Verify the degree of preservation compliance required for your project using Table 1 in the Preservation Content Specifications Implementation Guidance (PCS IG) document. [DS] 
Three levels of expected compliance are defined: Complete, Partial and Basic, based on different program types. With a few exceptions (e.g. CSDA), most IMPACT projects will be categorized as ‘Partial.’ If your project is under a different program or funding mechanism, follow that program’s preservation requirements.  
B1.5.1b Organize access to the preservation content collected as required for the compliance degree identified. [DS] 
Necessary content is identified in Table 2 of the PCS IG document. At a minimum, the following information should be collected for all IMPACT projects including CSDA:
Instrument and Platform Description, which should be included in DMP;
Metadata, which should be embedded in the data product files;
Product quality checks, flags and other quality information, which should be included in the data product guide;
Published articles that use the data products;
Final checklist that identifies the preservation content for each data product. 

Note the final preservation item is a deliverables checklist consisting of links to the collected documents.
1.6 Data Monitoring


Requirements

A1.6.1 Collect/monitor/report data usage metrics
A1.6.2 Collect/monitor data quality metrics
A1.6.3 Monitor data ingest and/or production metrics, as appropriate
A1.6.4 Conduct periodic review of metrics

Procedures

B1.6.1a Implement metrics collection procedures (see Requirement A1.1.13 for metrics implementation plan)  [DE]
B1.6.1b Produce on a routine basis the metrics reports and distribute, quarterly reports recommended [DE]
The same procedures also apply to A1.6.2 and A1.6.3 
B1.6.4 Review metrics routinely to assess operational performance [DS + DE]


2. Metadata

2.1 Planning and Design

Requirements

A2.1.1 Adhere to a standard metadata schema for data product (collection) and file (granule) level metadata
A2.1.2 Support mandatory metadata elements in the selected schema
A2.1.3 Utilize a standard data product naming convention
A2.1.4 Identify any needed additional metadata fields for specific projects
A2.1.5 Incorporate any access control fields into the metadata, as required
A2.1.6 Define the data product citation

Procedures

B2.1.1 Utilize the UMM or STAC schema [DS + DE]
UMM schema: UMM Schemas 
STAC Spec: STAC Spec
B2.1.2a UMM: Ensure that the metadata schema includes the following: [DS]
UMM Collection-Level mandatory fields: Metadata Specification, Entry Title, DOI, Abstract, Data Center, Processing Level, Collection Progress, Science Keywords, Temporal Extents, Spatial Extent, Platform, Related URL, Short Name, Version 
UMM Granule-Level mandatory fields: Granule UR, Provider Dates, Collection Reference, Metadata Specification
B2.1.2b STAC: Ensure that the metadata schema includes the following: [DS]
STAC Collection-Level mandatory fields: type, stac_version, id, description, license, extent, links
STAC Granule(Item)-Level mandatory fields: type, stac_version, id, geometry, bbox, properties, links, assets
B2.1.3 Use the ARC guidelines when developing a data product naming convention - “The Entry Title should be a descriptive, formal title of the dataset. The Entry Title should not be the same as the Short Name element. It is recommended that the Entry Title follow a mixed case capitalization scheme and that the use of special characters (such as underscores) and acronyms be kept to a minimum, if possible. In order to make titles descriptive, important elements about the data may be included, such as: parameters measured, geographic location, instrument, project, temporal coverage, etc.” Link for more info https://wiki.earthdata.nasa.gov/display/CMR/Entry+Title [DS]
B2.1.4a UMM: Review entire schema and determine what optional elements are needed. Where available, use existing optional elements. If needed, leverage custom UMM Additional Attribute fields. For example, the MAAP project developed SAR specific elements that were implemented as Additional Attributes. [DS]
B2.1.4b STAC: Review the available STAC extensions. [DE?] Extensions should be selected using the following criteria: 
Relevance to the data being described. Projection, item assets, electro-optical (EO), raster and SAR are commonly used extensions. 
Maturity of the STAC extension. Only mature extensions (pilot, candidate or stable) should be utilized.
B2.1.5a UMM: Utilize the ‘Access Constraints’ element to define constraints as needed. https://wiki.earthdata.nasa.gov/display/CMR/Access+Constraints [DS]
B2.1.5b TBD: STAC: The best practice for providing Access Constraints using the STAC Collection spec is still TBD. [DS + DE]
B2.1.6 Work with the data provider, the principal investigator (PI), or others to come to a consensus on how the data product should be cited. This should include identifying the main contributors to the data product, the order of the contributors in the citation and the data product name. [DS]

Citation should include:
Author(s) or project name(s): The people or organizations responsible for the intellectual work to develop the data product
Date published: When the particular version of the data set was first made available for use
Title: The formal title of the data set
Release version: Current data version
Repository: The name of the entity that holds, archives, publishes, distributes, or releases the data
DOI: Resolvable persistent identifier that provides the ability to access data
Access date: Date of online data were accessed
Resource type: [dataset]
	
Sources: AGU Data and Software Citation; ESIP Data Citation Guidelines for Earth Science Data



2.2 Generation / Curation


Requirements
A2.2.1 Provide the metadata specification information in the collection level metadata
A2.2.2 Provide the data product title in the collection level metadata
A2.2.3 Provide the DOI information in the collection level metadata
A2.2.4 Provide the abstract information in the collection level metadata
A2.2.5 Provide information on the organization responsible for originating, processing, archiving, and/or distributing the dataset in the collection level metadata 
A2.2.6 Provide the data processing level in the collection level metadata
A2.2.7 Provide the data product production status in the collection level metadata
A2.2.8 Provide science keywords applicable to the data product in the collection level metadata
A2.2.9 Provide the temporal extent of the data in the collection level metadata
A2.2.10 Provide the spatial extent of the data in the collection level metadata
A2.2.11 Provide the platform information (and optionally, instrument information) in the collection level metadata
A2.2.12 Provide resource related URLs in the collection level metadata
A2.2.13 Provide a unique identifier for the data product in the collection level metadata
A2.2.14 Provide the data product version in the collection level metadata
A2.2.15 Provide the data product license information in the collection level metadata   
A2.2.16 Provide a unique identifier in the granule level metadata
A2.2.17 Provide a date indicating when the granule metadata was created in the granule level metadata
A2.2.18 Provide a reference to the associated collection in the granule level metadata
A2.2.19 Provide the metadata specification information in the granule level metadata 
A2.2.20 Provide the temporal extent information in the granule level metadata
A2.2.21 Provide the spatial extent information in the granule level metadata
A2.2.22 Provide a link to access the data in the granule level metadata
A2.2.23 Preserve data provenance (preserve metadata from original source, and data editing history)

Procedures

B2.2.1a: UMM: Populate the ‘MetadataSpecification’ element. The Metadata Specification element requires the user to add in schema information into every collection record. It includes the schema's name, version, and URL location. [DS] More details can be found here: https://wiki.earthdata.nasa.gov/display/CMR/Metadata+Specification+for+UMM-C 
B2.2.16b: STAC: Provide a value of “Collection” in the STAC ‘type’ element and the version of STAC the Collection implements in the ‘stac_version’ element. [DS + DE]  
B2.2.2a UMM: Populate the ‘EntryTitle’ element. The Entry Title should be a descriptive, formal title of the dataset (see B2.1.3). [DS] More details can be found here: https://wiki.earthdata.nasa.gov/display/CMR/Entry+Title  
B2.2.2b STAC: Provide the data product title in the ‘title’ element. [DS + DE]
B2.2.3a UMM: Include the DOI itself and the authority information in the DOI element. The authority for most metadata is https://doi.org/. In most cases, a DOI should be provided (i.e. ‘MissingReason is not an option). More information on DOI best practices for UMM can be found here: https://wiki.earthdata.nasa.gov/display/CMR/DOI [DS]
B2.2.3b STAC: Leverage the Scientific Citation Extension Specification and the ‘sci:doi’ element within that specification. This element is meant to contain the DOI value of the data, e.g. 10.1000/xyz123. This MUST NOT be a DOI link. [DS + DE]
B2.2.4a UMM: Include the abstract, or dataset description, in the ‘Abstract’ element. The Abstract should provide a brief description of the resource the metadata represents.The abstract should summarize the dataset and mimic a journal abstract that is useful to the science community, but is also approachable for a first time user of the data. [DS] More information on abstract best practices for UMM can be found here: https://wiki.earthdata.nasa.gov/display/CMR/Abstract
B2.2.4b STAC: Include the abstract, or dataset description, in the ‘description’ element for the collection specification. [DS + DE]
B2.2.5a UMM: Include information on the organization responsible for originating, processing, archiving, and/or distributing the dataset in the ‘DataCenter’ element. The organization name must be selected from the GCMD provider list. The role of the organization must be selected from the following values: DISTRIBUTOR, ORIGINATOR, ARCHIVER, PROCESSOR. ‘NASA/IMPACT’ is a project in the GCMD provider list. This enumeration should be used for UMM Data Center information when IMPACT is the originator of the data. Selection of the role will depend on the individual project’s needs. [DS] For example, for the HLS data products, the role of NASA/IMPACT is ‘PROCESSOR.’ More details on the Data Center options and best practices for UMM can be found here: https://wiki.earthdata.nasa.gov/display/CMR/Data+Center 
B2.2.5b STAC: Include the data center information in the ‘providers’ element [Provider Object] using the Collection specification. When possible, use the GCMD provider list enumeration in the ‘name’ element of the Provider Object. The ‘roles’ element should also be provided. Roles in STAC are: licensor, producer, processor or host. [DS]
B2.2.6a UMM: Provide the data processing level in the ‘ProcessingLevel’ element. The data processing level should align with the EOSDIS data processing level whenever possible. [DS] More details on the EOSDIS data processing level can be found here: https://earthdata.nasa.gov/earth-science-data-systems-program/policies/data-information-policy/data-levels. More details on data processing level best practices can be found here: https://wiki.earthdata.nasa.gov/display/CMR/Processing+Level
B2.2.6b STAC: Provide the processing level using the ‘processing:level’ element using the processing extension (https://github.com/stac-extensions/processing) for Collections. [DS + DE]
B2.2.7a UMM: Provide the collection production status in the ‘CollectionProgress’ element. Leverage the Collection Progress controlled vocabulary to describe the production status of the dataset. [DS] More info and the controlled vocabulary list can be found here: https://wiki.earthdata.nasa.gov/display/CMR/Collection+Progress  
B2.2.7b TBD: STAC: The best practice for providing the collection progress using the STAC Collection spec is still TBD. [DS + DE]
B2.2.8a UMM: Populate science keywords using the ‘ScienceKeywords’ element. Science keywords should represent the scientific parameters being provided in the data as well as any broader conceptual terms that may aid in describing the data. [DS] The Science Keywords are chosen from a controlled keyword hierarchy. More information on science keyword best practices can be found here: https://wiki.earthdata.nasa.gov/display/CMR/Science+Keywords 
B2.2.8b STAC: The ‘keywords’ element in the Collection specification should be used to provide science keywords. Science keywords should be selected from the GCMD science keywords list. [DS + DE]
B2.2.9a UMM: Dates provided in CMR metadata should comply with the ISO 8601 Standard, which is an International Standard for the representation of dates and times. Select from one of three options in the UMM for describing the temporal extent of data: Single Date Time, Range Date Time and Periodic Date Time. More specific details on each of these options can be found here: https://wiki.earthdata.nasa.gov/display/CMR/Temporal+Extents [DS + DE]
B2.2.9b STAC: The temporal information should be provided in the Extent Object -> Temporal Extent Object. If the data has an open range (e.g. active/ongoing collection), this can be specified by setting the start and/or end time to “null”. For example, [["2019-01-01T00:00:00Z", null]]. Timestamps should consist of a date and time in UTC and MUST be formatted according to RFC 3339, section 5.6. The temporal reference system is the Gregorian calendar. [DS + DE] 
B2.2.10a UMM: Determine what type of spatial extent is being described in the metadata. In the CMR, there is the option to describe the horizontal, vertical, and orbital spatial coverage of a dataset along with its coordinate system and resolution. [DS] More details on each type can be found here: https://wiki.earthdata.nasa.gov/display/CMR/Spatial+Extent
For horizontal spatial extents, select an option that best describes the coverage of the data product. The four options in the CMR include: point, bounding rectangle, gpolygon and line. More details can be found here: https://wiki.earthdata.nasa.gov/display/CMR/Spatial+Extent
Determine whether any other spatial extent metadata (horizontal spatial resolution, geodetic model, etc) should be included in the metadata.

B2.2.10b STAC: The spatial information should be provided in the Extent Object -> Spatial Extent Object. The first bounding box always describes the overall spatial extent of the data. All subsequent bounding boxes can be used to provide a more precise description of the extent and identify clusters of data. The coordinate reference system of the values is WGS 84 longitude/latitude. [DS + DE]
B2.2.11a UMM: Provide the platform information using the ‘Platform’ element. Ensure all relevant platforms are listed in the metadata. [DS] More details can be found here: https://wiki.earthdata.nasa.gov/display/CMR/Platform. If applicable, it is also recommended that instruments/sensors be provided as well using the ‘Instrument’ element. More details can be found here: https://wiki.earthdata.nasa.gov/display/CMR/Instrument. The Platform name(s) must be selected from the GCMD Platform keyword list and Instrument/Sensor names must be selected from the GCMD Instruments keyword list. If a platform is not listed, a new keyword may be requested through the GCMD Keywords Community Forum. 
B2.2.11b TBD: STAC: Provide platform and optionally instrument information using the ‘summaries’ element in the Collection spec. Instructions on how to appropriately do this is TBD (example: https://github.com/radiantearth/stac-spec/blob/master/examples/collection.json#L45) [DS + DE]
B2.2.12a UMM: Provide links to all relevant documentation in the collection level metadata. A complete list of relevant documentation can be found here: https://wiki.earthdata.nasa.gov/display/CMR/Related+URLs. Any documentation created in either the ‘data’ ‘code,’ or ‘information content’ activities should also be included here. Describe each URL using the guidance provided here: https://wiki.earthdata.nasa.gov/display/CMR/Related+URLs [DS]
B2.2.12b STAC: Provide links to all relevant information in the Link Object -> ‘href’ and ‘rel’ elements. Provide the actual link in the ‘href’ element. The ‘ref’ element describes the relationship between the collection and the URL. More information on relationships can be found here. [DS + DE]
B2.2.13a UMM: A unique identifier for the data product should be provided in the ‘ShortName’ element. This should take the form of a shortened or abbreviated name of the data product. [DS] More details on short name best practices can be found here: https://wiki.earthdata.nasa.gov/display/CMR/Short+Name
B2.2.13b STAC: A unique identifier for the data product should be provided in the ‘id’ element. [DS + DE] 
B2.2.14a UMM: Provide the data product version number in the collection level metadata using the ‘Version’ element. [DS] More detailed information can be found here: https://wiki.earthdata.nasa.gov/display/CMR/Version 
B2.2.14b TBD: STAC: The best practice for providing the data product version using the STAC Collection spec. [DS + DE]
B2.2.15a UMM: Provide the license information in the ‘UseConstraints’ element. At a minimum, it is recommended that a link to the relevant license be provided in the ‘UseConstraints/LicenseURL/Linkage’ field. [DS] Additional info and other options for providing the license information can be found here: https://wiki.earthdata.nasa.gov/display/CMR/Use+Constraints 

Note: If a EULA identifier is available, linked it to the UMM-C under the ‘UseConstraints/EULAIdentifiers’ field.

B2.2.15b STAC: Provide the license using the ‘license’ element. The license should be selected from the SPDX License identifier list: https://spdx.org/licenses/. Provide “varies” if multiple licenses apply and “proprietary” for all other cases. The default value for an open license should be “CC0-1.0” unless otherwise specified. [DS + DE]
B2.2.16a UMM: Provide the universal reference identifier of the granule in each metadata record using the ‘GranuleUR’ element. Typically, the Granule UR will match the granule's file name. However, other identifying information may be included. The UR must be completely unique when compared to all other data from the same provider (i.e. datasets with the same provider Id in CMR). [DS] More details can be found here: https://wiki.earthdata.nasa.gov/display/CMR/Granule+UR  
B2.2.16b STAC: Provide a unique identifier for the file/layer/granule in the STAC Item spec ‘id’ field. [DS + DE]
B2.2.17a UMM: At least one provider date is required for granule metadata in the CMR. The ‘ProviderDates’ element presents dates associated with changes made to the granule in the database where it is stored. The Provider Dates include the date the granule was created, inserted, or updated in its database, and the date the granule metadata will be deleted from the CMR. It is highly recommended that the ‘Create’ date be included. [DS] More details can be found here: https://wiki.earthdata.nasa.gov/display/CMR/Provider+Dates 
B2.2.17b STAC: Provide the date the Item record was created using ‘properties > created’ https://github.com/radiantearth/stac-api-spec/blob/main/stac-spec/item-spec/common-metadata.md#date-and-time [DS + DE]
B2.2.18a UMM: The Collection Reference identifies the collection to which the granule belongs. This is done by providing the collection's Short Name and Version Id, or the collection's Entry Title in the ‘CollectionReference’ element. [DS] More details can be found here: https://wiki.earthdata.nasa.gov/display/CMR/Collection+Reference 
B2.2.18b STAC: Provide the id of the associated STAC Collection in the ‘collection’ element.  [DS + DE]
B2.2.19a UMM: Populate the ‘MetadataSpecification’ element. The Metadata Specification element requires the user to add in schema information into every granule record. It includes the schema's name, version, and URL location. [DS] More details can be found here: https://wiki.earthdata.nasa.gov/display/CMR/Metadata+Specification 
B2.2.19b: STAC: Provide a value of “Feature” in the STAC ‘type’ element and the version of STAC the Item implements in the ‘stac_version’ element. [DS + DE]
B2.2.20a UMM: File level temporal extent information is not required per UMM but is highly recommended to enable more targeted discovery. Ensure file level temporal information does not contradict collection level information. [DS] More details can be found here: https://wiki.earthdata.nasa.gov/display/CMR/Temporal+Extents
B2.2.20b STAC: The temporal information should be provided in the Properties Object -> ‘datetime’ element. If the file can be represented as a single datetime, the time stamp can be provided directly in the ‘datetime’ element. If the temporal extent is a temporal range, then a value of “null” should be provided in ‘datetime’ followed by the ‘start_datetime’ and ‘end_datetime’ elements. Timestamps should consist of a date and time in UTC and MUST be formatted according to RFC 3339, section 5.6. [DS + DE]
B2.2.21a UMM: File level spatial extent information is not required per UMM but is highly recommended to enable more targeted discovery. For specific files or granules, the spatial extent describes the area covered by that individual file. Ensure file level spatial information does not contradict collection level information. [DS] More details can be found here: https://wiki.earthdata.nasa.gov/display/CMR/Spatial+Extent
B2.2.21b STAC: The spatial information should be provided using either the ‘geometry’ element or the ‘bbox’ element. Use ‘geometry’ to provide a GeoJSON footprint, formatted according to RFC 7946, section 3.1. The footprint should be the default GeoJSON geometry, though additional geometries can be included. Coordinates are specified in Longitude/Latitude or Longitude/Latitude/Elevation based on WGS 84. If ‘geometry’ is set to “null”, then providing a ‘bbox’ is required. The bbox should be formatted according to RFC 7946, section 5. [DS + DE]
B2.2.22a UMM: A link to access the data described by the granule metadata is not required per UMM but is highly recommended to enable data access. This link is provided in the ‘RelatedURLs’ element. The link goes in the ‘RelatedURLs -> URL’ field, and this must be accompanied by a ‘RelatedURLs -> Type’ value of “GET DATA”. An optional ‘Subtype’ can be provided to further describe the nature of the URL. It is recommended that the link also be accompanied by a description. [DS] Additional details can be found here: RelatedURLs (Granules)
B2.2.22b STAC: There are two required STAC elements to consider here. First is the ‘links’ element which is a list of Link Objects to resources and related URLs. At minimum, a link with the ‘rel’ set to “self” is strongly recommended. It is encouraged that links pointing to other related STAC resources are also provided, such as a link to the associated collection and root or parent entity. Additional information can be found here: Link Object. The second is the ‘assets’ element which is a dictionary of Asset Objects that can be downloaded, each with a unique key. This element is key to facilitating data access, as it contains a URI to data associated with the Item that can be downloaded or streamed. Details can be found here: Asset Object [DS + DE]
B2.2.23a TBD: UMM: Implementation procedure for data provenance.
B2.2.23b TBD: STAC  Implementation procedure for data provenance    


2.3 Sharing


Requirements
A2.3.1 Make metadata available to ensure data are findable 

Procedures
B2.3.1 Ensure metadata is available either by pushing the metadata to a centralized catalog or by making it available for STAC catalog to index. [DS + DE]

2.4 Use / Reuse

Requirements
A2.4.1 Provide license information

Procedures
B2.4.1 If there are data access restrictions, ensure that the licensing description is provided. If using a standard license, ensure a link to the license is provided in the metadata. [DS + DE]

2.5 Preservation

Requirements
A2.5.1 Ensure the metadata record and dataset landing page is always accessible when a data set is being preserved (Ex: end of project, going from active to static)
A2.5.2 Ensure metadata meets a defined level of quality and remains up to date 
A2.5.3 If data is retired and no longer accessible, ensure the metadata record and dataset landing page is updated to indicate the data is retired. The metadata and landing page should remain accessible. If a new version of the data is available, the landing page should point to the new version.

Procedures
B2.5.1a Test to ensure the metadata record and dataset landing page are accessible [DE]
B2.5.1b Ensure that the metadata record and dataset are included in the archived information package (AIP). An AIP is the set of content and metadata to be archived at and managed by a preservation repository. [DS]
B2.5.2a Regularly make needed changes to the metadata in order to ensure a baselined level of quality. At a minimum, all mandatory metadata fields should pass pyQuARC quality checks. Additional manual quality checking can be conducted on an as-needed basis to ensure the information in the metadata is up to date. [DS]
B2.5.2b Add content to new elements added as a part of any schema changes. Update existing metadata fields as needed to ensure schema compliance. [DS]
B2.5.3a For data no longer accessible, update metadata record to contain retired information and new version availability if applicable. [DS]
B2.5.3b For data no longer accessible, test to ensure the metadata record and dataset landing page remain accessible [DS + DE]


2.6 Monitoring


Requirements
A2.6.1 Monitor and report metadata quality metrics
A2.6.2 Monitor changes to the metadata schema used
A2.6.3 Monitor changes to controlled vocabularies
Procedures
B2.6.1 Use pyQuARC to generate metadata quality metrics. QuARC is also available via an API. [DE]
B2.6.2a Update the schema to the latest version. More information on UMM schemas can be found here. More information on the STAC specification can be found here. [DE]
B2.6.2b Update the content for each metadata record to be compliant with the new schema changes [DS+DE]
B2.6.3a Update controlled vocabulary keywords in metadata (e.g., GCMD; CF); [DS]
B2.6.3b Remove deprecated keywords, as needed [DS + DE]



3. Digital Content

3.1 Planning and Design


Requirements
A3.1.1 Determine the information that is needed to describe and use the data
A3.1.2 Define how the information will be retained
A3.1.3 Define the object citation

Procedures
B3.1.1 Identify the following minimum information needed to support the use of the data and plan/schedule how this information will be produced (e.g. gather from existing resources, create new documents, etc.). [DS] This includes information on:
How the data was collected or generated. This includes Processing and Algorithm Version History, Data Product Generation Algorithms, Data Product Quality, Science Data Product Software, Science Data Product Algorithm Inputs, Science Data Product Validation. Most of this information can be included in the ATBD using APT but more detailed documents may be required depending on the use case. 
How to use the data (Science Data Access and Analysis Tools such as user guides, jupyter notebooks and code), 
Important contextual information (Instrument and Platform Description, Instrument and Science Team Information, On-orbit, in-flight, in-the-field calibration methods, project/mission/field campaign)
Any Disclaimer information including EULA or licensing agreements text as required
Any project specific acknowledgement text as required
Note that gathering and maintaining this information also ensures the project meets any preservation requirements.
B3.1.2a Create a retention plan for documentation and information / digital content, including what to retain and where it will be stored [DS]
Use agreed-upon document repository for baselining and storing documents for your project 
B3.1.2b Ensure that the scope for the Archived Info Package (AIP) described in B2.5.1b includes information and digital content generated about the data [DS]
B3.1.3 Work with the creators and others to come to a consensus on how the object should be cited. This should include identifying the main contributors to the object, the order of the contributors in the citation and the object name. [DS]



3.2 Generation / Curation


Requirements
A3.2.1 Gather or create the information needed to describe and use the data

Procedures
B3.2.1 Gather, or create if not available, the minimum information to support the use of the data as described in B3.1.1 [DS]


3.3 Sharing


Requirements
A3.3.1 All the information and documents identified in B3.2.1 should be openly accessible 
A3.3.2 All the web content should have semantic annotations 
A3.3.3 All the documents created for the information identified in B3.2.1 should be assigned a persistent identifier
A3.3.4 Ensure documents are assigned an open license

Procedures
B3.3.1a Ensure all information and documentation are made publicly accessible using this Zenodo best practices guide [DS + DE]
B3.3.1b Ensure all web content is 508 compliant in order to be accessible to those with disabilities (508 compliance checkers: Microsoft documents; PDF in Adobe Acrobat; Web Pages; Checklist). [DS + DE]
B3.3.2 Ensure all web pages use RDFa Lite for semantic annotations [DE]
B3.3.3 If not assigned by NASA, obtain a Zenodo account and publish your documents to Zenodo to get a DOI [DS]
B3.3.4a Select ‘open access’ for the Access rights in Zenodo. Open access documents have higher visibility in Zenodo. [DS]
B3.3.4b  Leverage the Creative Commons Attribution 4.0 International license in Zenodo for open access documentation. [DS]
B3.3.4c Ensure the documents are added to the IMPACT community in Zenodo. [DS] 


3.4 Use/ Reuse


Requirements
A3.4.1 Use a clear data versioning scheme to ensure that users are aware of significant changes to documents

Procedures
B3.4.1 Provide users with information on major and minor changes to documentation. [DS]

3.5 Preservation
Requirements
A3.5.1 Preserve all the information associated with the data product
A3.5.2 For retired data products, provide web content with the statement that the data product is no longer available and provide a link to a newer or relevant data product if possible

Procedures
B3.5.1 Ensure that information (documents) identified in B3.2.1 are included in the Archived Info Package described in B2.5.1b [DS + DE]
B3.5.2 Update the webpage so that the metadata is retained and the DOI continues to resolve to an information page [DE]


3.6 Monitoring
Requirements
A3.6.1  Develop a plan to review and maintain information and documentation to ensure they are up-to-date.
A3.6.2 Monitor usage statistics of documents

Procedures
B3.6.1 Conduct annual check of documents through automated or manual means ( schedule annual documentation review for new or updated information, script to check links in documentation, etc.) [DS+DE]
B3.6.2 Leverage Zenodo metrics or equivalent to understand document views and usage. Zenodo metrics include [DS]
Views
Downloads
Unique views
Unique downloads
Citations


4. Code

4.1 Planning and Design
Requirements
A4.1.1	Identify relevant code associated with the data 
A4.1.2 Identify where the code will be hosted 
A4.1.3 Select an open, permissive code license in line with open science requirements
A4.1.4 Identify or develop a plan for how the repository and contributions will be managed long term. The plan should include a code of conduct and guidelines for contributing to the public repository.
A4.1.5 Identify the open source process required for the organization 
A4.1.6 Define the software citation

Procedures
B4.1.1a Create an inventory of the source code and/or libraries. This should include code used to create the data or code used to read, use, analyze or visualize the data. [DS +DE]
B4.1.1b Determine which code are not restricted and are available for open release [DE]
B4.1.2 Select a source code versioning repository. IMPACT should use GitHub. [DE]
B4.1.3 Use a permissive license with broad acceptance in the science community to share your code. This includes Apache License 2.0, the BSD 3-Clause “Revised” License, and the MIT License (ESDS Open Source Software Policy | Earthdata (nasa.gov)). Note that in the past, most IMPACT projects have used the Apache License 2.0. If submitting through the NASA center process, the Center Intellectual Property Counsel may provide guidance on which license to select. [DE]
B4.1.4a Determine if the code should be public where public is defined as code with no expectations for maintenance or accepting contributions versus open source where open source is traditional open source software code with external contributions. Code should always at a minimum be public. [DS + DE]	
B4.1.4b If the code is being managed as an open source repository, a plan should be developed that describes how contributions will be managed, how frequently the repo will be checked for contributions, how contributions will be checked for quality, etc…[DS + DE]
B4.1.4c A code of conduct for the repository should be provided. Use the Github code of conduct until NASA SMD guidance is available [DS + DE] 
B4.1.5 Ensure that NASA’s Open Source paperwork has been submitted before code is written. Note that IMPACT is required to open source any software that is developed that is funded by NASA. Use the IMPACT Open Source Software process template based on the process outlined below: 
Login to https://softwarerelease.ndc.nasa.gov/
Select “Create a new software release package”
Click https://invention.nasa.gov/ and select Report your New NTR
You will need Project information including a description and members
Submit for approval
Once approved your new software will appear in https://softwarerelease.ndc.nasa.gov/dashboard/
Fill out all the required details and submit for approval: you will need to zip the source code to upload
You will get an email from the MSFC OSS group
Save that email and periodically ask for status [DS]
B4.1.6 Work with the code provider, the PI and others to come to a consensus on how the code should be cited. This should include identifying the main contributors to the code, the order of the contributors in the citation and the code name. [DS]



4.2 Generation / Curation


Requirements
A4.2.1 Develop code in accordance with current best practices
A4.2.2 Ensure code is citable 

Procedures
B4.2.1 Use the IMPACT coding best practices [DE]
B4.2.2a Create a clear, sufficiently descriptive name for your code repo [DE]
B4.2.2b Create a citation file for all code with information identified in B4.1.6. [DS]


4.3 Sharing


Requirements
A4.3.1 Ensure that the code is openly accessible 
A4.3.2 Ensure that the code has a persistent identifier and is discoverable with the data
A4.3.3 Ensure the code is documented 

Procedures
B4.3.1Ensure the code repository is set to ‘public’ in GitHub. [DE]
B4.3.2a The code repository should be assigned a registered persistent identifier. Use Zenodo for assigning a new DOI. In rare cases that a DOI has been assigned via other mechanisms, ensure the DOI is uploaded to the repo. [DS]
B4.3.2b Ensure the code identifier is added to the data product metadata. [DS]
B4.3.2c Ensure the DOI is added to the Github citation file [DS + DE]
B4.3.3a Include a read me document that describes the purpose of the code and any system requirements [DE]
B4.3.3b Include a brief ‘About’ description of the code that will be displayed towards the top of the repo page. The description should be no more than 325 characters or 50 words. [DS]

4.4 Use / Reuse

Requirements
A4.4.1 Ensure a recommended citation is provided for the code
A4.4.2 Ensure code use constraints are provided
Procedures
B4.4.1 Upload the citation file to the GitHub repository and ensure the ‘Cite This Repository’ link is functioning. A citation is also provided on the Zenodo page. 
B4.4.2 Ensure the license is included in the GitHub repository.


4.5 Preservation

Requirements
A4.5.1 Preserve the code associated with the data product.

Procedures
B4.5.1a Code for generating data products should be included in the AIP [DS]
B4.5.1b Code for reading data products should be included in the AIP [DS]


4.6 Monitoring


Requirements
A4.6.1 Generate relevant metrics related to the code repository
A4.6.2 Ensure users have access to the code

Procedures
B4.6.1 Generate periodic metrics to include: [DE]
GitHub Analytics
Views
Downloads
Commits
Number of contributors
Number of followers
Number of forks
Tracking activity of Issues and Comments
Number of referring sites
B4.6.2 Conduct periodic check of repositories through automated or manual means to ensure code is open and accessible [DE]

Resources 
5. Storage

5.1 Planning and Design
Requirements
A5.1.1 Define storage metrics to be collected
A5.1.2 Develop storage and egress cost estimates 
A5.1.3 Define the S3 Bucket structure including the naming convention, tagging, logs, etc.
A5.1.4	Define data storage environments needed such as sandbox, development, staging, production (operational + backup copies)
A5.1.5 Define storage policy for retention including the rules for defining which storage class to use and when the storage class should be changed 
A5.1.6 Define storage policy for retiring data

Procedures
B5.1.1 Implement bucket inventories and associated detailed metrics [DE]
B5.1.2 Use data sheet (B1.1.4) to generate storage and egress costs using this calculator. Costs to consider include total product size, whether redundant storage for operations and backup/archive are require], projected growth, and estimated use[DE]
B5.1.3 Adopt these best practices for key aspects of cloud storage [DS + DE]
B5.1.4 Recommend creating a minimum of three storage environments - sandbox, development/staging and production. [DE]
B5.1.5 For a rolling archive with capped storage capacity, use information in the best practices document. [DE]
If selected metrics show that the usage for a dataset is below a threshold, then the dataset should be moved to cold storage or deleted
Storage class can be changed manually (s3 cli) or by setting bucket specific rules. These rules or changes should be cost based (or time based) - need to determine at what usage it becomes more costly to store and retrieve from “colder” storage class versus keeping data in “warmer class” (i.e. <~40% of total dataset egress it becomes cheaper to use IA vs standard storage)
B5.1.6 The retired dataset should be moved to cold storage or deleted [draft CSDA retirement document [DE]


5.2 Generation / Curation

Requirements
A5.2.1 Create the S3 Bucket structure
A5.2.2	Create the data storage environments indicated in the storage plan (A5.1.4) 
A5.2.3 Create storage policy for retention rules 
A5.2.4 Create AWS policies for controlling the volume of data that end users or applications may download.

Procedures
B5.2.1 Use the best practices for key aspects of the S3 bucket structure including the naming convention, tagging, logs, etc. [DE]
B5.2.2 Create the different buckets based on the storage plan A5.1.4 [DE]
B5.2.3 Create the retention rules using the AWS (link to tutorial) [DE]
B5.2.4 Create the egress throttling rules using AWS [DE]



5.3 Sharing
Requirements
A5.3.1 Develop access permissions for different S3 buckets (sandbox, prod etc)
Procedures
B5.3.1 Implement ACLs for S3 Buckets using AWS [DE]

5.4 Use / Reuse (N/A)

5.5 Preservation 

Requirements
A5.5.1 Adhere to storage policy for retiring data
Procedures
B5.5.1 The retired dataset should be moved to cold storage or deleted [DE]

5.6 Monitoring
Requirements
A5.6.1 Develop cost alerts to inform responsible parties when certain thresholds are exceeded
A5.6.2 Monitor storage usage and develop storage alerts to inform responsible parties when thresholds are exceeded

Procedures
B5.6.1 Use CloudWatch | Kion (formally cloud tamer) to monitor cost and set up alerts [DE]
B5.6.2 Use CloudWatch to monitor storage use and set up alerts [DE]

6. People
6.1 Planning and Design


Requirements
A6.1.1 Schedule training needs for data stewards and data engineers

Procedures
B6.1.1 Develop onboarding materials for use in training data stewards and data engineers [DS+DE]
Common:
Introduction to Data Management and Sharing
Open Science 101 (coming December 2023)
Explore Earthdata Website and Wiki
Data stewards:
Metadata training (coming soon)
SPD-41a and  - open-source science guidance document
Data Engineers
Basic cloud (AWS) training